{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L8p_yzVc37J9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div>\n",
    "    <img width=\"100%\" src=\"media/cover.png\">\n",
    "    <center>\n",
    "        <h1>Life After Word Embeddings:<br/>Language modeling for transfer learning in NLP</h1>\n",
    "        <h3>Julian Eisenschlos @eisenjulian</h3>\n",
    "    </center>\n",
    "    <table width=\"100%\"><tr aligh=\"left\">\n",
    "      <td><img width=\"100\" src=\"https://storage.googleapis.com/m-infra.appspot.com/public/botmaker/bmtop7.png\"/></td>\n",
    "      <td align=\"right\">23 de Noviembre de 2018 - LIAA</td>\n",
    "    </tr></table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THHCVUqtBzwl",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "- Transferencia de aprendizaje\n",
    "- Modelado de lenguaje\n",
    "- Fine-tuned Transformer Language Model\n",
    "- Univeral Language Model Fine-Tuning (ULM-FiT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Slides\n",
    "\n",
    "<center>\n",
    "    <h3><a href=\"https://bit.ly/transfer-nlp\">bit.ly/transfer-nlp</a></h3>\n",
    "    <img style=\"height: 400px;\" src=\"media/qr.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L55nPn7VBNgE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transferencia de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wjgm3f2JTYJY",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Aprendizaje Supervisado\n",
    "\n",
    "Aproxima una función $f: X\\to Y$ a partir de un conjunto de entrenamiento $T \\subset X\\times Y$ de forma de minimizar una función de error \n",
    "\n",
    "$$loss(f) = \\frac{1}{|T|}\\sum_{(x,y)\\in T} \\ell(f(x), y)$$\n",
    "\n",
    "Donde $\\ell: Y\\times Y\\to \\mathbb{R}$ es una función no negative de pérdida puntual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4G8On66bYLOq",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Limitaciones del aprendizaje supervisado\n",
    "\n",
    "- Cantidad de datos de entrenamiento\n",
    "- El dilema de bias vs. variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"media/bias-variance.png\" alt=\"bias_variance\" style=\"height: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7v0rzkTaoj2",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Una breve historia de Computer Vision\n",
    "\n",
    " - En 2006, *Fei-Fei Li* , profesora de Illinois, insipirada por *WordNet* decide crear [ImageNet](https://www.researchgate.net/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database)\n",
    " - En 2009 gracias a Mechanical Turk, se publica el dataset con **3.2 millones** images en **5,247** categorías\n",
    " - En 2012 Hinton, Sutskever y Krizhevsky publican [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) mejorando el SOTA por **40%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![AlexNet](media/AlexNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vGPRLhJffPHS",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Representaciones intermedias\n",
    "\n",
    "Una red profunda se puede pensar como una composición de transformaciones diferenciables que factorizan a $f:A\\to C$ cómo composición de $2$ o más funciones.\n",
    "\n",
    "![f1f2](https://colah.github.io/posts/2015-09-NN-Types-FP/img/types-compose.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqHiZgJolVkr",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Un ejemplo\n",
    "\n",
    "Las conjuntos no son linealmente separables, pero con varias capas\n",
    "<center>\n",
    "<img src=\"https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/spiral.1-2.2-2-2-2-2-2.gif\" alt=\"simple_data\" style=\"\n",
    "    height: 350px;\n",
    "\">\n",
    "<label>Fuente <a href=\"https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\">Neural Networks, Manifolds, and Topology</a></label>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lg6gW51smBAD",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ¿Y la transferencia?\n",
    "\n",
    "Estas representaciones intermedias son reusables para otros inputs y otros outputs\n",
    "\n",
    "![arrows](https://colah.github.io/posts/2015-09-NN-Types-FP/img/types-branchmerge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El problema de transfer learning es el  **olvido catastrófico**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Sf6idOWKGxs",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vectores de palabras\n",
    "\n",
    "Los vectores de palabras $W:words\\to\\mathbb{R}^n$ son el output de la primera capa de una red que tenga palabras como features:\n",
    "\n",
    "\n",
    "$$W(perro) = (0.1, 0.3, -0.4, ...)$$\n",
    "$$W(animal) = (0.7, -0.2, -0.1, ...)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Embedding image](https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/embeddings.gif?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0rhgPOqueLr",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "! pip install --quiet gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6tebT0wmxXOD",
    "outputId": "5e75f26e-d7ff-4b89-eb02-c4f67fbcc5ea",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02412866,  0.05822411, -0.00761765,  0.01267011], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a FastText model\n",
    "model = FastText(common_texts, size=4, window=3, min_count=1, iter=10)\n",
    "\n",
    "# Get the vector for a word\n",
    "model.wv['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1 class=\"huge\">3% de mejora en performance</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h3>\n",
    "    ¿Qué otros conociemientos se pueden transferir?\n",
    "</h3></center>\n",
    "\n",
    "Desafíos cómo [GLUE](https://gluebenchmark.com/) y [decaNLP](https://github.com/salesforce/decaNLP) intentan encontrar técnicas comunes para resolver varios problemas en simultaneo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3n2tx_cOKJom",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modelado de lenguaje\n",
    "\n",
    "Un modelo de lenguaje es una distribución $P(w_1,  w_2,  \\cdots ,w_T )$ sobre las secuencias de palabras en $V^n$ en el conjunto de vocabulario $V$. \n",
    "\n",
    "Vía la regla de la cadena y la hipótesis de *Markov*, podemos aproximar $P$ como el producto de la probablidad de cada palabra dadas las $n$ anteriores:\n",
    "\n",
    "$$P(w_1,⋯,w_T)\\simeq\\prod_ip(w_i|w_{i−1},\\cdots,w_{i−n})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6CfPebVxDJdG",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ¿En castellano?\n",
    "\n",
    "Si tengo posibles oraciones, ¿cuál es la más probable?\n",
    "\n",
    "$$P(``\\textrm{Tengo que ir al banco}\") > P(``\\textrm{Tengo que ir al asiento}\")$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "biRmKP706CD3",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Aplicaciones\n",
    "\n",
    "En principio todo lo que sea generación de lenguage\n",
    "- Traducciones\n",
    "- Auto-corrector\n",
    "- Reconocimiento de audio\n",
    "- Chatbots\n",
    "- Question Answering\n",
    "- Resúmenes\n",
    "- Y ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SttG5cdf5-3M",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ejemplos\n",
    "\n",
    "El ejemplo más básico no paramétrico son los modelos basados en $n$-gramas, usando frecuencias de las partes de la oración:\n",
    "\n",
    "$$P(w_t|w_{t−1},\\cdots,w_{t−n})=\\frac{count(w_{t−n},\\cdots,w_{t−1},w_t)} {count(w_{t−n},\\cdots,w_{t−1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6x4QyxDbgtZQ",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los ejemplos paramétricos más comunes se basan en un embedding $v:V\\to\\mathbb{R}^k$, y una función (en general una red neuronal) $f: (w_{t-1}, \\cdots, w_{t-n})\\to \\mathbb{R}^k$ que dado el **contexto** construye un vector $h$ y con una capa *softmax* cacula:\n",
    "\n",
    "$$P(w|w_{t−1},\\cdots,w_{t−n})\\propto \\exp(h\\cdot v_w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGPF4DJ4n1YA",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Una arquitectura común para modelos esto es el uso de redes recurrentes (o **RNN**), es decir una función recursiva $g$ tal que:\n",
    "\n",
    "$$f(w_{t-1},\\cdots,w_{t-n}) = g(w_{t-1}, g(w_{t-2}, \\cdots))$$\n",
    "\n",
    "![RNN](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
    "\n",
    "Referencia: [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0JvTIwN46Fp1",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Perplejidad\n",
    "\n",
    "¿Cómo evaluamos si un modelo de lenguaje es bueno? \n",
    "- La mejor forma es con evaluaciones **extrínsicas** sobre las aplicaciones, sea traducciones, auto-corrector, etc... pero eso es caro\n",
    "- La medida standard **intrínsica** es la de **minimizar** la [perplejidad](https://en.wikipedia.org/wiki/Perplexity), el exponente de la **cross-entropy**, que mide cuan bien nuestro modelo aproxima la distribución empírica\n",
    "\n",
    "$$\\exp\\left(\\frac{1}{|C|}\\sum_{(w_1, \\cdots, w_n)\\in C} -\\log\\left(P\\left(w_n|w_{n-1},\\cdots,w_1\\right)\\right)\\right)$$\n",
    "\n",
    "Como referencia, una variable aleatoria de perplejidad $k$ tiene el mismo nivel de incerteza que un dado de $k$-caras.\n",
    "\n",
    "[//]: <> (Esto es equivalente a minimizar)\n",
    "[//]: <> ($$\\sum_{(w_1, \\cdots, w_n)\\in C} \\log\\left(\\sum_{w\\in V}\\exp f\\left(w_1,\\cdots,w_{n-1}\\right)\\cdot v_w\\right) - f\\left(w_1,\\cdots,w_{n-1}\\right)\\cdot v_{w_n}$$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cJLakfU3mGUH",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los dos datasets más usados para medir perplejidad son **Pen Tree Ban** y **Wikipedia**. Sobre Penn, con un vocabulario de $10,000$ palabras tenemos:\n",
    "\n",
    "|Modelo|Perplejidad|\n",
    "|-|-|\n",
    "|$5$\\-gramas|141|\n",
    "|LSTM vainilla|82|\n",
    "|[AWD-LSTMs](https://arxiv.org/abs/1708.02182)|57|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-qC8BBwCGwA",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Cómo generamos texto con modelo de lenguaje? Veamos un ejemplo en [PyTorch](https://github.com/pytorch/examples/blob/master/word_language_model/generate.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = torch.load('lm.pt')\n",
    "seed_words = \"so it was n't quite what I was expecting , but I really liked it ! The best\".split()\n",
    "\n",
    "# Input is a PyTorch recurrent model\n",
    "def init_model(model, seed_words):\n",
    "    hidden = model.init_hidden(1)\n",
    "    input = torch.tensor([[0]], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Iterate through the seed words to set the model state\n",
    "    for word in seed_words:\n",
    "        print(word, end=' ')\n",
    "        word_idx = corpus.dictionary.word2idx[word]\n",
    "        input.fill_(word_idx)\n",
    "        output, hidden = model(input, hidden)\n",
    "        \n",
    "    return hidden\n",
    "    \n",
    "hidden = init_model(model, seed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nxiYI0X8i57z",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def sample_model(model, hidden, temperature=1.0, length=50):\n",
    "    input = torch.tensor([[0]], dtype=torch.long).to(device)\n",
    "\n",
    "    # Generate new words based on the language model and the temperature\n",
    "    for i in range(length):\n",
    "        output, hidden = model(input, hidden)\n",
    "        word_weights = output.squeeze().div(temperature).exp().cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        input.fill_(word_idx)\n",
    "        word = corpus.dictionary.idx2word[word_idx]\n",
    "        print(word, end=' ')\n",
    "\n",
    "sample_model(model, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kw6V_gZnw4o2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "> so it was n't quite what i was expecting , but i really liked it ! the best ... film ever ! <eos> i saw this movie at the toronto international film festival . i was very impressed . i was very impressed with the acting . i was very impressed with the acting . i was surprised to see that the actors were not in the movie . \n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eHT4casqTGx0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Más aplicaciones\n",
    "\n",
    "¡Gracias a los modelos de lenguaje, todo el texto de internet puede ser el **ImageNet** para NLP!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jnUx_S4Wyj3f",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Cómo usamos el modelos de lenguaje para mejorar la performance en tareas cómo clasificación?\n",
    "\n",
    "![text_classification](media/class.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fine-tuned Transformer Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El modelo [Transformer](https://arxiv.org/abs/1706.03762) de *Vaswani et al* fue introducido en **2017**. Se basa principalemente en selt-attention basada en contenido y posición.\n",
    "\n",
    "![attention](media/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La atención es la versión diferenciable de acceder a un dato en un **key-value storage**\n",
    "\n",
    "$$\\textrm{Attention}\\left(Q,K,V\\right)=\\textrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V$$\n",
    "\n",
    "![multihead](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/MultiHead.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Originalmente usaron esta arquitectura para traducción, mejorando el **SOTA** para el WMT Benchmark de traducciones del inglés al alemán y al francés. Ver el código en [github](https://github.com/tensorflow/tensor2tensor)\n",
    "\n",
    "![universal-transformer](media/transformer.gif)\n",
    "\n",
    "Nota: esta imagen es de una siguiente iteración llamada [Universal Transformer](https://arxiv.org/abs/1807.03819)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A mitad de 2018, *Alec Radford et al* usaron transformers pre-entrenados usando modelos de lenguaje para muchas otras tareas clásicas de NLP con resultados muy buenos en casi todas\n",
    "\n",
    "![task-arq](media/openai.png)\n",
    "\n",
    "Se puede ver el código y el modelo entrenado en https://github.com/openai/finetune-transformer-lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Gwj_IvTVMgj",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Universal  Language Model Fine Tuning (ULM-FiT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j4U1pPenyRe8",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Jeremy Howard** y **Sebastian Ruder** propusieron ULM-FiT a principios de **2018**, con la idea de aplicar eficientemente modelos de lenguaje como base para otras tareas de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Las ideas principales son:\n",
    "- Partir de un modelo de lenguaje genérico\n",
    "- Re-entrenarlo en el dominio particular de mis datos (inclusive si no están todos etiquetados!)\n",
    "- Usar la última capa del modelo para otras tareas independientes con 2 capas lineales + ReLu sobre el output de la LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Las técnicas principales son:\n",
    "\n",
    "- [AWD-LSTMs](https://github.com/salesforce/awd-lstm-lm) = LSTM clásico + **magias para regularizar**\n",
    "  - DropConnect: Dropout a fijo a lo largo de todas las iteraciones de la red recursiva\n",
    "  - Una variante de Averaged Stochastic Gradient Descent\n",
    "- Gradual Unfreezing\n",
    "- Discriminative Fine Tuning\n",
    "- Slanted Triangular Learning Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Con $100$ ejemplos se logran resultados que antes necesitaban $20$ mil!\n",
    "\n",
    "![results](media/ulmfit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Resultados\n",
    "\n",
    "![results](media/ulmfit-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Y alcanza con Wikipedia en cualquier idioma para entrenarlo\n",
    "\n",
    "```bash\n",
    "$LANG  =  'es'\n",
    "\n",
    "# Install Fast.ai it might take a bit\n",
    "git clone https://github.com/fastai/fastai.git && cd fastai && pip install .\n",
    "cd fastai/courses/dl2/imdb_scripts/\n",
    "\n",
    "# Get spacy in your language\n",
    "pip install spacy\n",
    "python -m spacy download $LANG\n",
    "\n",
    "bash prepare_wiki.sh\n",
    "python pretrain_lm.py data/wiki/$LANG/ --cuda-id 0 --lr 1e-3 --cl 12\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FeWmhj0Ns4cT",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```bash\n",
    "# We just need train and val csv files as well a pretrained language model\n",
    "python fastai/courses/dl2/imdb_scripts/create_toks.py $DATA_PATH --lang $LANG\n",
    "python fastai/courses/dl2/imdb_scripts/tok2id.py $DATA_PATH\n",
    "python fastai/courses/dl2/imdb_scripts/finetune_lm.py $DATA_PATH wt103/$LANG --cuda-id 0 --cl 25\n",
    "python fastai/courses/dl2/imdb_scripts/train_clas.py $DATA_PATH --cuda-id 0 --cl 50\n",
    "```\n",
    "\n",
    "[Complete docs](https://github.com/fastai/fastai/tree/master/courses/dl2/imdb_scripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pasando en limpio\n",
    "\n",
    "- Los modelos de lenguaje, ¿pueden ser el **ImageNet** para NLP?\n",
    "- OpenAI Transformer Language Model\n",
    "  - Se basa en self-attention en lugar de RNNs\n",
    "  - Entrena conjuntamente el LM junto con la tarea específica\n",
    "  - Consigue mejores resultados en otras tareas más allá de clasificación\n",
    "  - Lento de entrenar (1 mes en 8 GPUs)\n",
    "- ULMFiT\n",
    "  - Se basa modelos de lenguaje AWD-LSTMs\n",
    "  - Usa learning rates discriminativos\n",
    "  - Hay datos uniformes en casi todos los idiomas\n",
    "  - Más rápido de entrenar\n",
    "- ¡Ambos modelos y código son públicos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Próximos pasos\n",
    "\n",
    "- Liberar los modelos pre-entrenados en muchos más idiomas\n",
    "- Aplicar las misma arquitectura a otras tareas además de clasificación\n",
    "- Entrenar modelos de lenguaje en varios idiomas simultaneamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5vDJY_DCMtt",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Referencias\n",
    "\n",
    " - [Regularizing and Optimizing LSTM Language Models\n",
    "](https://arxiv.org/abs/1708.02182) by *Merity*  et al introdues **AWD-LSTM** Networks\n",
    " - [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by *Vaswani* et al introduces the **Transformer**\n",
    " - [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146) by *Ruder* and *Howard* introduces **ULMFiT**\n",
    " - [Improving Language Understanding\n",
    "by Generative Pre-Training](https://blog.openai.com/language-unsupervised/) by *Radford* introduces a transformer LM-based geneeral NLP framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vTIrCSgVBM90",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <div height=\"300\">\n",
    "        <h1>¿Preguntas?</h1>\n",
    "        <h3>Julian Eisenschlos @eisenjulian</h3>\n",
    "    </div>\n",
    "    <br/><br/><br/><br/><br/><br/>\n",
    "    <footer>\n",
    "        <table width=\"100%\"><tr aligh=\"left\">\n",
    "          <td><img width=\"100\" src=\"https://storage.googleapis.com/m-infra.appspot.com/public/botmaker/bmtop7.png\"/></td>\n",
    "          <td align=\"right\">23 de Noviembre de 2018 - LIAA</td>\n",
    "        </tr></table>\n",
    "    </footer>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "TransferLearningNLP.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
