{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9Z_hiWFmK9i",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Clasificación de texto en TensorFlow\n",
    "===\n",
    "\n",
    "![text_classification](https://cdn-images-1.medium.com/max/700/1*HgXA9v1EsqlrRDaC_iORhQ.png)\n",
    "\n",
    "<center>\n",
    "  <b>Julian Eisenschlos</b>  @eisenjulian\n",
    "</center>\n",
    "<table width=\"100%\"><tbody><tr aligh=\"left\">\n",
    "      <td><img width=\"100\" src=\"https://storage.googleapis.com/m-infra.appspot.com/public/botmaker/bmtop7.png\"></td>\n",
    "      <td align=\"right\">20 de Octube de 2018 - DevFest Buenos Aires</td>\n",
    "</tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VrgUsV30n9cJ",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![qr-code](https://storage.googleapis.com/m-infra.appspot.com/public/eidico/frame.png)\n",
    "\n",
    "## Repo: bit.ly/dev-fest-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NAOnpFe361_3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definición del problema\n",
    "\n",
    "Dado un documento $d\\in D$ y clases $C = \\left\\{c_1,\\dots, c_k\\right\\}$ determinar una function $f:D\\to C$ que asigne categorías. \n",
    "\n",
    "Lo que queremos es armar un algortimo para producir un clasificador $f_T$ asociado a un set de entrenamiento:\n",
    "\n",
    "$$T \\subset D\\times C$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "---\n",
    "\n",
    "* ¿Cómo representamos los documentos? \n",
    "* ¿Conjuntos o secuencias?\n",
    "* ¿Letras, palabras, unidades semánticas?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WjDROD0KmWKc",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ejemplos más comunes\n",
    "\n",
    "* Detección de Spam\n",
    "* Lenguaje adulto / insultos / moderación de contenido\n",
    "* Análisis de sentimiento / opinión\n",
    "* Asignación de temas\n",
    "* Identificación de lenguaje\n",
    "* Reconocimiento de intenciones\n",
    "* Otros..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SFtO6FJKevIM",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Los datos\n",
    "\n",
    "Vamos a trabajar sobre un dataset en castellano etiquetado con la polaridad de reviews. En la tesis de [Luciana Dubiau](http://materias.fi.uba.ar/7500/Dubiau.pdf) encontramos dos datasets, uno de reviews de restaurants y otro de apps móbiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "O21VC63FPoHj",
    "outputId": "eaa6d58b-be4f-4000-f51d-3de621332858",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version 1.12.0-rc1\n"
     ]
    }
   ],
   "source": [
    "#@title Importamos las librerias necesarias\n",
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import json\n",
    "import collections\n",
    "import random\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorboard import summary as summary_lib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "model_dir = tempfile.mkdtemp()\n",
    "print('tf version', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "Mn7X4JlCPto3",
    "outputId": "b79a99c5-5b07-411c-b886-dada9b0c5500",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"el mozo atendio barbaro, los platos riquisimos y muy abundantes, el vino me encanto.. el lugar barbaro, con algo de penumbra y velas, muy bueno. el precio un poco salado, es un lugar chico, pero muy lindo.\", \n",
      "  \"Muy linda ambientaci\\u00f3n. Excelente atenci\\u00f3n y comida.  Porciones generosas. Precios adecuados. Seguro volver\\u00e9.\", \n",
      "  \"Muy bien preparada las mesas en la vereda con calefacci\\u00f3n y reparo . Atenci\\u00f3n de espumantes antes del servicio.Las pastas servidas en platos originales y calientes, y en su punto justo ademas de ricas y caseras. Muy amable  el servicio de camareros.Para recomendar y volver\"\n",
      "]"
     ]
    }
   ],
   "source": [
    "#@title Descargamos el dataset\n",
    "if not os.path.exists('restaurante-review-dataset.zip'):\n",
    "    ! wget -q http://web.fi.uba.ar/~ldubiau/datasets/restaurante-review-dataset.zip\n",
    "if not os.path.exists('restaurante-review-dataset'):\n",
    "    ! unzip -q restaurante-review-dataset.zip\n",
    "! cat restaurante-review-dataset/pos/9778.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "8mgeYRlVj2wg",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def tokenize(doc):\n",
    "    return doc.translate(translator).lower().split()\n",
    "\n",
    "def parse_file(path):\n",
    "    with open(path, 'r') as json_data:\n",
    "        return json.loads(json_data.read())\n",
    "\n",
    "def get_docs(folder):\n",
    "    full_path = os.path.join('restaurante-review-dataset', folder, '*.json')\n",
    "    return (doc for path in glob.glob(full_path) for doc in parse_file(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "lZpIxOezqh--",
    "outputId": "be08f217-dab1-4a3d-e080-de6ecc74552d",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 34808 pos docs, with an average of 233 tokens each.\n",
      "We have 17633 neg docs, with an average of 347 tokens each.\n",
      "There are in total 67174 distinct tokens.\n"
     ]
    }
   ],
   "source": [
    "#@title Algunas estadísticas sobre el dataset\n",
    "        \n",
    "def print_stats(folder):\n",
    "    total_docs = sum(1 for doc in get_docs(folder))\n",
    "    total_tokens = sum(sum(1 for token in doc) for doc in get_docs(folder))\n",
    "    print('We have {} {} docs, with an average of {} tokens each.'.format(\n",
    "        total_docs, folder, int(total_tokens/total_docs)))\n",
    "  \n",
    "def get_tokens():\n",
    "    return (token for doc in get_docs('*') for token in tokenize(doc))\n",
    "  \n",
    "print_stats('pos')\n",
    "print_stats('neg')\n",
    "\n",
    "print('There are in total {} distinct tokens.'.format(len(set(get_tokens()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ohLx7Ssol58",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Distribución de términos\n",
    "\n",
    "Podemos ver la distribución del vocabulario\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "colab_type": "code",
    "id": "T_gtpPD80DqV",
    "outputId": "dcb1e039-1998-4e0b-8846-26bb2f26af18",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#@title ¿Cuales son los términos más frecuentes?\n",
    "#@markdown `counter = collections.Counter(get_tokens())`\n",
    "#@markdown `most_common = counter.most_common(30)`\n",
    "\n",
    "counter = collections.Counter(get_tokens())\n",
    "most_common = counter.most_common(30)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "y_pos = np.arange(len(most_common)) * 1.2\n",
    "plt.bar(y_pos, [pair[1] for pair in most_common], align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, [pair[0] for pair in most_common], rotation=45, ha='right', fontsize=14)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Most frecuent tokens') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![freq](https://raw.githubusercontent.com/eisenjulian/nlp_estimator_tutorial/master/token_weights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MsaLtqP4sQ2b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Armando el train y test set\n",
    "\n",
    "Construímos un vocabulario con las palabras más frecuentes de modo de mapear los tokens a índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5vA05Lmk8H-",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "word_index = {token[0]: i+1 for i, token in enumerate(counter.most_common(10000))}\n",
    "word_inverted_index = {v: k for k, v in word_index.items()}\n",
    "index_offset = 3\n",
    "vocab_size = 5000\n",
    "\n",
    "def doc_to_index(doc, start=1, oov=2, offset=index_offset, size=vocab_size):\n",
    "    return np.array(\n",
    "        [start] + \n",
    "        [word_index[token] + offset \n",
    "         if token in word_index and (word_index[token] + offset < size)\n",
    "         else oov for token in tokenize(doc)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "QG9_twFBvtkQ",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#@markdown Y aplicando esta función tenemos nuestro set de entrenamiento\n",
    "\n",
    "\n",
    "pos = [doc_to_index(doc) for doc in get_docs('pos')]\n",
    "neg = [doc_to_index(doc) for doc in get_docs('neg')]\n",
    "x_all = pos + neg\n",
    "y_all = [1] * len(pos) + [0] * len(neg)\n",
    "random.Random(42).shuffle(x_all)\n",
    "random.Random(42).shuffle(y_all)\n",
    "\n",
    "train_size = int(len(x_all) * 0.7)\n",
    "\n",
    "x_train_variable = x_all[:train_size]\n",
    "y_train = np.array(y_all[:train_size])\n",
    "x_test_variable = x_all[train_size:]\n",
    "y_test = np.array(y_all[train_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BCO6ivC9t_U2",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La dimensionalidad se reduce achicando el vocabulario y asignando el término *Fuera de vocabulario* o *OOV* al resto\n",
    "\n",
    "* ¿Cómo elegir qué palabras dejar y cuales eliminar?\n",
    "  * Las $N$ más frecuentes, funciona bien en el 90% de los casos\n",
    "  * Las $N$ más \"informativas\" de cada clase, usando la información mutua\n",
    "  * Descartar *stopwords* (de, la, con, en, y, etc...)\n",
    "* ¿Tamaño del vocabulario?\n",
    "  * Depende el algoritmo, hay algunos como SVMs que son cuadráticos en la cantidad de features\n",
    "* ¿Qué hacer con las palabras que se quedan afuera?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UY8iWgyOV486",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Baselines\n",
    "\n",
    "El primer baseline siempre debe ser el casificador constante, con el que obtenemos una exactitud de $66\\%$. Vamos a comenzar con dos modelos bien simples pero efectivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FaURBiWu84Sp",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Naive Bayes\n",
    "\n",
    "El primero es el de Naive Bayes, basado en el modelo gráfico que describe a los features cómo condicionalmente independientes entre sí dentro de cada clase.\n",
    "\n",
    "![NB](http://rspa.royalsocietypublishing.org/content/royprsa/465/2109/2927/F1.large.jpg?width=800&height=600&carousel=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Podemos modelar la aparición de palabras dentro de cada documento dentro de una clase $c\\in C$ de dos formas\n",
    "* Cada palabra $w$ aparece o no según una variable de Bernoulli con probabilidad $p_{c,w}$\n",
    "* La frecuencia de las palabras dentro de un documento corresponde a reiteradad evaluaciones de una distribución multinomial con probalidades $p_{c,w}$ con $\\sum_{w\\in V} p_{c,w} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El desarrollo del *MAP* es similar en ambos casos. Para la distribución de Bernoulli tenemos\n",
    "\n",
    "$$P(c|x) = \\frac{P(c)\\times P(x|c)}{P(x)} \\propto P(c) \\times \\prod_{w\\in V} P(x_w|c) = P(c) \\times \\prod_{w\\in V} (x_w p_{c,w} + (1-x_w)(1-p_{c,w}))$$\n",
    "\n",
    "y si tomamos logaritmo llegamos a que \n",
    "\n",
    "$$\\log P(c|x) = \\log P(c) + \\sum_{w\\in V} (x_w \\log p_{c,w}  + (1-x_w)\\log(1-p_{c,w})) + \\dots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Por otro lado en el caso de la distribución multinomial el resultado es similar\n",
    "\n",
    "$$P(c|x) \\propto P(c) \\times \\frac{(\\sum_w x_w)!}{\\prod_w x_w!} \\times \\prod_{w\\in V} p_{c,w}^{x_w}$$\n",
    "\n",
    "y de nuevo tomando logaritmos\n",
    "\n",
    "$$\\log P(c|x) = \\log P(c) + \\sum_{w\\in V} x_w \\cdot \\log(p_{c,w}) + \\dots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**¿Les suenan la pinta de estos modelos?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "9H1Pl97AHjyv",
    "outputId": "ee000609-b270-401a-840e-c258f171740f",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial accuracy: 0.9519481344943749\n",
      "Bernoulli   accuracy: 0.9038327083200915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "def get_counts(x):\n",
    "    counts = np.zeros((len(x), vocab_size), dtype=int)\n",
    "    for i, doc in enumerate(x):\n",
    "        for j in doc:\n",
    "            counts[i][j] += 1\n",
    "    return counts\n",
    "\n",
    "x_train_counts = get_counts(x_train_variable)\n",
    "x_test_counts = get_counts(x_test_variable)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "print(\"Multinomial accuracy: {}\".format(mnb.fit(x_train_counts, y_train).score(x_test_counts, y_test)))\n",
    "print(\"Bernoulli   accuracy: {}\".format(bnb.fit(x_train_counts, y_train).score(x_test_counts, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TZAHFkGpTsJs",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dada lo absurdo de la hipótesis de NaiveBayes ¿por qué funcionan?\n",
    " * La distribución de probabilidad que da es usualmente muy mala, pero sólo nos interesa que discrimine bien\n",
    " * Muy robustas a ruido\n",
    " * Muy robustas a cambios en el tiempo\n",
    " \n",
    "Además son:\n",
    " * Un gran baseline para clasificación de texto\n",
    " * Estúpidamente rápida para \"entrenar\" y predecir\n",
    " * Modelos compactos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GgFfawAIWCTM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vectores por documento\n",
    "\n",
    "Otro ejemplo de muy útil es el algoritmo de **Centroide más cercano** o [Rocchio](https://nlp.stanford.edu/IR-book/html/htmledition/rocchio-classification-1.html), que calcula un prototipo de cada clase como el promedio de sus vectores y después clasifica con el vector más cercano.\n",
    "\n",
    "![Rocchio](https://nlp.stanford.edu/IR-book/html/htmledition/img1091.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cómo NaiveBayes, este es un caso particular de modelo lineal.\n",
    "\n",
    "En general usa vectores unitarios pesados con *tf-idf* y la distania coseno para comparar. Algunos puntos importantes:\n",
    " * Si la clase es polimórfica o disjunta es le complica\n",
    " * Es útil para muchas clases con pocos ejemplos por cada una como clasificación de intenciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Una mejora sobre este clasificador es el de $k$-Nearest-Neighbors que vota la clase para un ejemplo $x$ en base a la moda de los $k$ más cercanos. Ambos estan disponibles en `sklearn`.\n",
    " * Escala muy bien para muchas clases\n",
    " * Soporta clases muy disjuntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Wr7p01pajKm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimadores lineales\n",
    "\n",
    "Para seguir trabajando sobre modelos más complejos vamos a mudarnos a `TensorFlow` y armar un pipeline para recorrer nuestro training data. Vamos a usar tres componentes de la lbrería\n",
    "\n",
    "* `tf.data.Dataset`\n",
    "* `tf.feature_colum`\n",
    "* `tf.estimator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "VI8WD3e-5JvL",
    "outputId": "4951e776-1b96-4b08-d7d2-174ee2274af5",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (36708, 400)\n",
      "x_test shape: (15733, 400)\n"
     ]
    }
   ],
   "source": [
    "sentence_size = 400\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "x_train = sequence.pad_sequences(x_train_variable, \n",
    "                                 maxlen=sentence_size, \n",
    "                                 padding='post', \n",
    "                                 value=0)\n",
    "x_test = sequence.pad_sequences(x_test_variable, \n",
    "                                maxlen=sentence_size, \n",
    "                                padding='post', \n",
    "                                value=0)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xni8Vcft6UD",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h4>Los datasets son el puente entre nuestros datos en disco o la red al runtime de python</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRhwkf_v5ZcD",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_len_train = np.array([min(len(x), sentence_size) for x in x_train_variable])\n",
    "x_len_test = np.array([min(len(x), sentence_size) for x in x_test_variable])\n",
    "\n",
    "def parser(x, length, y):\n",
    "    features = {\"x\": x, \"len\": length}\n",
    "    return features, y\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, x_len_train, y_train)) \\\n",
    "      .shuffle(buffer_size=len(x_train_variable)) \\\n",
    "      .batch(100) \\\n",
    "      .map(parser) \\\n",
    "      .repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_test, x_len_test, y_test)) \\\n",
    "      .batch(100) \\\n",
    "      .map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGsghi_CWwS-",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h4>Los features columns son el puente entre la data en memoria y la representación numérica que los modelos saben procesar</h4>\n",
    "\n",
    "\n",
    "![feature_columns](https://www.tensorflow.org/images/feature_columns/some_constructors.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhZR8aoKXQWs",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "column = tf.feature_column.categorical_column_with_identity('x', vocab_size)\n",
    "\n",
    "embedding_size = 50\n",
    "word_embedding_column = tf.feature_column.embedding_column(column, dimension=embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OinPaV1JXVKN",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Los estimadores vienen en dos sabores</h3>\n",
    "\n",
    "![alt text](https://www.tensorflow.org/images/custom_estimators/estimator_types.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "p9IkYRHkElWF",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#@markdown Código para calcular la curva PR\n",
    "\n",
    "def draw_pr_curve(classifier):\n",
    "    predictions = np.array([p['logistic'][0] for p in classifier.predict(input_fn=eval_input_fn)])\n",
    "\n",
    "#     # Plot the curve\n",
    "#     precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "#     plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "#     plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "#     plt.xlabel('Recall')\n",
    "#     plt.ylabel('Precision')\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "      \n",
    "    # Reset the graph to be able to reuse name scopes\n",
    "    tf.reset_default_graph() \n",
    "    # Add a PR summary in addition to the summaries that the classifier writes\n",
    "    pr = summary_lib.pr_curve('precision_recall', predictions=predictions, labels=y_test.astype(bool), num_thresholds=21)\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'), sess.graph)\n",
    "        writer.add_summary(sess.run(pr), global_step=0)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cH6qWnSK5d2x",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "all_classifiers = {}\n",
    "def train_and_evaluate(classifier):\n",
    "    all_classifiers[classifier.model_dir] = classifier\n",
    "    classifier.train(input_fn=train_input_fn, steps=5000)\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "    draw_pr_curve(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2288
    },
    "colab_type": "code",
    "id": "4Gvam1wm5nzk",
    "outputId": "223605ab-185e-49b4-e931-568291f053ee",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp43x26c8n/bow_sparse', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7febd0afbbe0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp43x26c8n/bow_sparse/model.ckpt.\n",
      "INFO:tensorflow:loss = 69.31472, step = 0\n",
      "INFO:tensorflow:global_step/sec: 74.2044\n",
      "INFO:tensorflow:loss = 21.007729, step = 100 (1.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.7284\n",
      "INFO:tensorflow:loss = 8.146741, step = 200 (1.019 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.5067\n",
      "INFO:tensorflow:loss = 16.00558, step = 300 (1.029 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.1994\n",
      "INFO:tensorflow:loss = 7.4020567, step = 400 (1.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.0912\n",
      "INFO:tensorflow:loss = 10.748516, step = 500 (1.027 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.4121\n",
      "INFO:tensorflow:loss = 12.495168, step = 600 (1.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.859\n",
      "INFO:tensorflow:loss = 10.820482, step = 700 (1.000 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.4879\n",
      "INFO:tensorflow:loss = 5.6009, step = 800 (1.071 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.598\n",
      "INFO:tensorflow:loss = 3.7857041, step = 900 (0.995 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.669\n",
      "INFO:tensorflow:loss = 6.2577443, step = 1000 (0.991 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.041\n",
      "INFO:tensorflow:loss = 7.9848595, step = 1100 (1.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.2129\n",
      "INFO:tensorflow:loss = 7.4159923, step = 1200 (1.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.419\n",
      "INFO:tensorflow:loss = 3.9935086, step = 1300 (0.992 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.077\n",
      "INFO:tensorflow:loss = 5.099376, step = 1400 (0.992 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.7261\n",
      "INFO:tensorflow:loss = 6.731308, step = 1500 (1.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.41\n",
      "INFO:tensorflow:loss = 1.7663412, step = 1600 (0.999 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.005\n",
      "INFO:tensorflow:loss = 5.359914, step = 1700 (0.999 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.323\n",
      "INFO:tensorflow:loss = 8.359367, step = 1800 (0.987 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.4018\n",
      "INFO:tensorflow:loss = 4.002772, step = 1900 (1.069 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.608\n",
      "INFO:tensorflow:loss = 8.010852, step = 2000 (0.993 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.3421\n",
      "INFO:tensorflow:loss = 7.0130033, step = 2100 (1.010 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.031\n",
      "INFO:tensorflow:loss = 5.6716766, step = 2200 (0.988 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.7418\n",
      "INFO:tensorflow:loss = 3.2564607, step = 2300 (1.069 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.617\n",
      "INFO:tensorflow:loss = 10.458032, step = 2400 (0.992 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.403\n",
      "INFO:tensorflow:loss = 4.022852, step = 2500 (0.987 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.1847\n",
      "INFO:tensorflow:loss = 3.8637862, step = 2600 (1.073 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.1959\n",
      "INFO:tensorflow:loss = 5.634002, step = 2700 (1.015 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.587\n",
      "INFO:tensorflow:loss = 5.6257687, step = 2800 (0.987 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.8758\n",
      "INFO:tensorflow:loss = 15.982742, step = 2900 (1.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.748\n",
      "INFO:tensorflow:loss = 4.712029, step = 3000 (1.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.814\n",
      "INFO:tensorflow:loss = 4.646823, step = 3100 (0.991 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.599\n",
      "INFO:tensorflow:loss = 2.5590043, step = 3200 (0.980 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.001\n",
      "INFO:tensorflow:loss = 3.2601478, step = 3300 (1.000 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.2919\n",
      "INFO:tensorflow:loss = 3.3306882, step = 3400 (1.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.367\n",
      "INFO:tensorflow:loss = 2.0442631, step = 3500 (0.998 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.738\n",
      "INFO:tensorflow:loss = 4.6585913, step = 3600 (0.992 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.4138\n",
      "INFO:tensorflow:loss = 3.1609788, step = 3700 (1.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.797\n",
      "INFO:tensorflow:loss = 4.9145412, step = 3800 (0.992 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.4403\n",
      "INFO:tensorflow:loss = 1.9302373, step = 3900 (1.010 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.3441\n",
      "INFO:tensorflow:loss = 6.738381, step = 4000 (1.007 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.6127\n",
      "INFO:tensorflow:loss = 5.3317285, step = 4100 (1.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.817\n",
      "INFO:tensorflow:loss = 4.2499356, step = 4200 (0.984 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.694\n",
      "INFO:tensorflow:loss = 3.9360197, step = 4300 (0.990 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.438\n",
      "INFO:tensorflow:loss = 2.7966425, step = 4400 (1.000 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.2687\n",
      "INFO:tensorflow:loss = 6.199197, step = 4500 (1.069 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.102\n",
      "INFO:tensorflow:loss = 2.617331, step = 4600 (0.991 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.578\n",
      "INFO:tensorflow:loss = 1.3874115, step = 4700 (0.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.8602\n",
      "INFO:tensorflow:loss = 2.4199266, step = 4800 (1.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.102\n",
      "INFO:tensorflow:loss = 3.2226334, step = 4900 (0.980 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/tmp43x26c8n/bow_sparse/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.1386375.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-10-20-02:37:55\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp43x26c8n/bow_sparse/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-10-20-02:37:57\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.96580434, accuracy_baseline = 0.66350985, auc = 0.9910399, auc_precision_recall = 0.9951315, average_loss = 0.10532457, global_step = 5000, label/mean = 0.66350985, loss = 10.487794, precision = 0.97432214, prediction/mean = 0.65681314, recall = 0.97413546\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /tmp/tmp43x26c8n/bow_sparse/model.ckpt-5000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp43x26c8n/bow_sparse/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.LinearClassifier(feature_columns=[column], model_dir=os.path.join(model_dir, 'bow_sparse'))\n",
    "train_and_evaluate(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSuC7n4Iccrd",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Uno de los beneficios de los predictores lineales es que son muy inspeccionables, podemos ver los features más relevantes con facilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "maZ5DyYD6xqx",
    "outputId": "7f6ef9cc-24d6-449c-dfc3-580406bf7fee",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAF+CAYAAACvRkIWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3XdcVfX/B/DXhSsblamm4kxFGe6R\nAweKMydpaqmVpZn1TU0zvykOnGnlSrMyt4iDUJYDQQQEQRFZigqCKBuUoczP749+3K+WJFzgyqHX\n8/H4Pr557+G8P59zzr2v8znryoQQAkRERFTrqb3uBhAREVHFMLSJiIgkgqFNREQkEQxtIiIiiWBo\nExERSQRDm4iISCIY2kQV1L59e3z++ed/e33ZsmVo37690vNNT0/HhQsXKvU3M2bMQGRkpNI1n//7\nRYsWwcbGBn5+flWeb3nu3buHq1evvvQ9Pz8/PHz48JXzaN++PZKTk6u7aUSSIn/dDSCSklu3biE3\nNxd6enoAgMLCQty8ebNK8wwKCkJAQACGDBlS4b/Zt29flWo+//dubm7w8vKCmZkZ+vfvX6X5luf8\n+fMoLi5Gjx49/vbe77//jrlz5+KNN96okdpEdQlH2kSV0KtXL5w7d07x78uXL8PS0vKFaTw8PDB6\n9GgMHz4c77//PhISEgAAt2/fxuTJkzFq1CgMGzYMBw8eRGRkJFatWgUvLy98+eWXf6tXNq8RI0Zg\nzJgxCAoKAgAMHjwYISEhAIBdu3ahT58+mDhxIg4dOoTBgwcDALZt24ZVq1Zh3rx5GDJkCCZNmoTU\n1NQX/v69995DaWkpPvzwQ/j6+r4wXxcXF9jZ2cHOzg5fffUVCgsLAQDOzs4YMWIEhg0bhmnTpiEp\nKQkAcPLkSXz++ef45ptvYGdnh5EjRyI2Nhbe3t7YvXs39u/fj/Xr17/Qvx9++AFXrlzBV199BXd3\ndxQUFGD58uWws7PDiBEjsH79epSUlPxtuXz//ff49NNPUVpaijt37mD69Omws7PDmDFjFDtRQUFB\nmDx5MjZv3owRI0Zg8ODBCA4OLnddEEmCIKIKadeunQgICBAffPCB4rUFCxaIS5cuiXbt2gkhhEhK\nShLdunUT8fHxQgghfv31VzFjxgwhhBDz588XJ0+eFEIIkZGRIebOnSsKCgrE1q1bxTfffPPSmr16\n9RIPHjwQQghx9epVsXbtWiGEEIMGDRJXr14Vt2/fFt26dRMpKSni2bNnYvr06WLQoEFCCCG2bt0q\n+vTpIx48eCBKS0vFxx9/LHbu3PnC35f169GjRy+8npiYKHr37i2Sk5NFaWmpmDdvntizZ49IT08X\nFhYWium//vprRdtPnDghrK2txc2bN4UQQjg4OIhly5YJIYRYsmSJ2LFjx0v7+Hxbdu/eLWbPni2K\niorE06dPxcSJE4WLi8sL7XRzcxPjx48XeXl5oqSkRAwbNkwcO3ZMCCFESEiI6NevnygqKhJXrlwR\nFhYW4ty5c0IIIfbs2SNmzpz5j+uCqLbjSJuoEnr27InY2FhkZGTg6dOnuH79Ovr06aN439/fH716\n9UKLFi0AAPb29ggKCkJxcTGMjIzg5eWFyMhIGBgYYOfOndDQ0PjHekZGRjh69CiSkpLQvXt3LF26\n9IX3r169ip49e8LU1BSampqYOHHiC+93794dTZs2hUwmg7m5OR49elShfvr7+6NLly5o1KgRZDIZ\nNm/ejJkzZ8LIyAihoaFo3LixYv6JiYmKv2vTpg0sLCwAAB07dqxwvTI+Pj545513IJfLoaWlhTFj\nxsDf31/xfmRkJH788Uf89NNP0NHRwb1795CRkYFJkyYBALp16wZDQ0Ncv34dAKCrqwtbW1sAQKdO\nnRTnzpVZF0S1AUObqBLU1dUxbNgweHh44OLFi+jXrx/k8v9dGpKVlYX69esr/q2vrw8hBLKysrBo\n0SK0a9cO//nPf2BjY4NDhw69st5PP/2E9PR0TJgwAePGjVMc3i3z5MkTNGjQQPHvRo0avfC+vr7+\nC21/2aHml/lrPzQ1NSGXy1FSUoKtW7di5MiRsLOzw/fffw/x3M8XKFuvTGZm5gv9adCgATIyMhT/\nXrFiheJ14M/+P3v2DCNGjMDw4cMxfPhwZGRkIDs7+2/tUVNTQ2lpKQAotS6IagOGNlEljRw5El5e\nXvD09MTIkSNfeM/IyEgRGADw+PFjqKmpwcDAALq6uliwYAHOnTuH7du3Y+vWrYiLi/vHWmZmZli3\nbh0CAwPx/vvvY+HChS+8r6enh/z8fMW/y85ZV5WBgQGysrIU/87NzUV6ejrc3d3h7e2NgwcPwsvL\n66VX01eFsbHxC8svOzsbxsbGin9v3rwZnTp1wnfffQcAMDU1ha6uLjw9PRX/u3z5MoYOHfqPdZRZ\nF0S1AUObqJK6dOmC1NRUxMbGomfPni+817dvX4SEhCgOGR89ehR9+/aFXC7HnDlzEBsbCwBo164d\n9PT0IJPJIJfLkZOT87c6mZmZmDVrFnJzc6GmpgZra2vIZLIXprGyskJQUBAyMzNRWFgIFxeXaumj\njY0Nrl27hgcPHkAIgRUrVuD48ePIyMhA06ZNYWhoiKysLHh4eCAvL++V8yuvj399b+DAgTh+/DhK\nSkqQn5+PP/74AzY2NoppW7RogW+//Raenp4ICgpC06ZN0bhxY3h6egL4c5ktWLDghR2ZlylvXRDV\ndrzli6iSZDIZhg4diqdPn0JN7cX93saNG2PNmjX49NNPUVRUhGbNmmH16tUAgOnTp2PhwoUoKioC\nAEydOhUtW7ZE3759sXfvXkycOBEnTpxQzMvQ0BD9+/fHxIkToa6ujnr16sHR0fGFelZWVhg/fjzG\njx+PJk2aYOTIkfj999+r3MfGjRtj1apVmDFjBtTV1WFpaYlZs2YhJycHbm5uGDp0KJo3b47//Oc/\nmDt3LtavX4927dqVO79BgwZh0aJFSEpKwtatW194z87ODgsWLMDnn3+O9957D4mJiRg1ahRkMhmG\nDx+OESNGvDC9gYEBVq5ciaVLl8LV1RVbtmyBg4MDfvjhB6ipqWHWrFnQ0dH5x/6Vty6IajuZEPw9\nbSIpE0IoRok+Pj744Ycfqm3ETUS1Cw+PE0lYZmYmevfujaSkJAgh4OHhgc6dO7/uZhFRDeFIm0ji\njhw5gt9++w0ymQytW7eGo6MjjIyMXneziKgGMLSJiIgkgofHiYiIJIKhTUREJBG1/pavtLSX39tZ\n0wwMdJCV9c/3erKO6mvUtTp1qS+qqlOX+qKqOnWpL3Wxzl+ZmOiX+x5H2uWQy9VZpxbWqGt16lJf\nVFWnLvVFVXXqUl/qYp3KYGgTERFJBEObiIhIIhjaREREEsHQJiIikgiGNhERkUQwtImIiCSCoU1E\nRCQRDG0iIiKJYGgTERFJBEObiIhIIhjaREREElHrfzCEiIiourj43avwtLq6msjLK3jldOP6t65K\nkypFpaH99OlTfP3118jIyEBBQQE+/fRTDBo0SJVNICIikiyVhvbFixdhYWGB2bNnIykpCR988AFD\nm4iIqIJUGtojR45U/PejR4/QqFEjVZYnIiKStNdyTnvKlClITk7Grl27Xkd5IiIiSZIJIcTrKBwd\nHY3FixfD1dUVMpms3OmKi0tq5Q+RExFR9TrsFVOt85tq16HGa5RXp6aodKQdEREBIyMjNGnSBObm\n5igpKUFmZiaMjIzK/ZusrHwVtvB/TEz0kZaWwzq1rEZdq1OX+qKqOnWpL6qqI5W+VORKbaDiV3W/\nrC0VrVHVOlVhYqJf7nsqvU87JCQEv/32GwAgPT0d+fn5MDAwUGUTiIiIJEuloT1lyhRkZmZi6tSp\n+Pjjj7F8+XKoqfH5LkRERBWh0sPjWlpa2Lx5sypLEhER1Rkc5hIREUkEQ5uIiEgi+OxxIiL6RxV9\nXndtfFZ3XcORNhERkUQwtImIiCSCoU1ERCQRDG0iIiKJYGgTERFJBEObiIhIIhjaREREEsHQJiIi\nkgg+XIWISKKq+6EnAB98UttxpE1ERCQRDG0iIiKJYGgTERFJBEObiIhIIhjaREREEsGrx4mIagB/\nzpJqAkObiP5VGKYkZQxtIvpHFQ05oGpBx3uOiV6N57SJiIgkgqFNREQkEQxtIiIiiWBoExERSQRD\nm4iISCIY2kRERBLB0CYiIpIIhjYREZFEMLSJiIgkgqFNREQkEQxtIiIiiWBoExERSQRDm4iISCJe\ny698bdy4EaGhoSguLsYnn3yCYcOGvY5mEBERSYrKQ/vKlSuIjY2Fk5MTsrKyMH78eIY2ERFRBag8\ntHv06AErKysAQP369fH06VOUlJRAXV1d1U0hIiKSFJWHtrq6OnR0dAAAx48fx4ABA/4xsA0MdCCX\nv55ANzHRZ51aWKOu1antfdHV1az26V/WlsrUqei0qqhT3nJVRR0us8rXUdX2XFNeyzltADh//jyO\nHz+O33777R+ny8rKV1GLXmRioo+0tBzWqWU16lodKfQlL6+gwtPq6mpWaPqXtaWidSpaQ1V1yluu\nqqjDZVb5Oqranqvin3YCXkto+/n5YdeuXfjll1+gr6+6PRQiIiIpU3lo5+TkYOPGjfj999/RsGFD\nVZcnIiKSLJWHtru7O7KysvCf//xH8dqGDRvwxhtvqLopREREkqLy0J48eTImT56s6rJERESSxyei\nERERSQRDm4iISCIY2kRERBLB0CYiIpIIhjYREZFEMLSJiIgkgqFNREQkEQxtIiIiiWBoExERSQRD\nm4iISCIY2kRERBLB0CYiIpIIhjYREZFEMLSJiIgkgqFNREQkEQxtIiIiiWBoExERSQRDm4iISCIY\n2kRERBLB0CYiIpIIhjYREZFEMLSJiIgkgqFNREQkEQxtIiIiiWBoExERSQRDm4iISCIY2kRERBLB\n0CYiIpIIhjYREZFEMLSJiIgkgqFNREQkEQxtIiIiiXgtoX379m3Y2tri4MGDr6M8ERGRJKk8tPPz\n87F69Wr06dNH1aWJiIgkTeWhraGhgT179sDU1FTVpYmIiCRNrvKCcjnkcpWXJSIikrxan54GBjqQ\ny9VfS20TE33WqYU16lqd2t4XXV3Nap/+ZW2pTJ2KTquKOuUtV1XU4TKrfB1Vbc81pdaHdlZW/mup\na2Kij7S0HNapZTXqWh0p9CUvr6DC0+rqalZo+pe1paJ1KlpDVXXKW66qqMNlVvk6qtqeq+KfdgJ4\nyxcREZFEqHykHRERgQ0bNiApKQlyuRxeXl7Ytm0bGjZsqOqmEBERSYrKQ9vCwgIHDhxQdVkiIiLJ\n4+FxIiIiiWBoExERSQRDm4iISCIY2kRERBLB0CYiIpIIhjYREZFEMLSJiIgkgqFNREQkEQxtIiIi\niWBoExERSQRDm4iISCIY2kRERBLB0CYiIpIIhjYREZFEMLSJiIgkgqFNREQkEQxtIiIiiWBoExER\nSQRDm4iISCIY2kRERBLB0CYiIpIIhjYREZFEMLSJiIgkgqFNREQkEQxtIiIiiWBoExERSQRDm4iI\nSCIY2kRERBLxytAOCgr622vnz5+vkcYQERFR+eTlvfHgwQMkJiZiw4YNWLJkieL14uJirF27Fra2\ntippIBEREf2p3NBOS0uDu7s7kpKSsHPnTsXrampqmDJlikoaR0RERP9Tbmh36dIFXbp0gY2NDUfV\nREREtUC5oV3GzMwMa9asQU5ODoQQitc3btxYow0jIiKiF70ytBcsWIBRo0ahU6dO1VJw7dq1uHHj\nBmQyGb755htYWVlVy3yJiIjquleGtqGhIebOnVstxYKDg3H//n04OTnh7t27+Oabb+Dk5FQt8yYi\nIqrryr3lq7S0FKWlpRg8eDD8/f1RWFioeK20tFSpYoGBgYrz423atMHjx4+Rm5urXMuJiIj+ZWTi\n+RPVz+nQoQNkMhle9rZMJkN0dHSli3377bcvXNg2depUODo6olWrVuX+TXFxCeRy9UrXKs9hr5hq\nm1eZqXYdaryOKmrUtTp1qS+qqvOyGkRUe5R7eDwmpvq/dP6qnP2FF2Rl5Vdrzby8ggpNp6urWeFp\n09JyarxOVWqoqg6XWeXrSGGZVYaJiX6V51EbatS1OnWpL3WxzsvqlueV57R//PHHv72mrq6O1q1b\nY/jw4VBTq/iTUE1NTZGenq74d2pqKkxMTCr890RERP9mr0zczMxMuLu7IycnB3l5efDy8kJycjJO\nnz6N//73v5Uq1rdvX3h5eQEAIiMjYWpqCj09PeVaTkRE9C/zypF2SkoKXFxcoK2tDQB4+vQpFi9e\njJ9++gnvvvtupYp17doVnTp1wpQpUyCTybBixQrlWk1ERPQv9MrQTk1NVQQ2AGhra+Phw4cAgIKC\nip8fLLNo0aJK/w0RERFVILStra1hb2+P7t27QyaT4caNG2jZsiVcXFxgYWGhijYSERERKhDaK1as\nQGBgIKKjo1FaWooPP/wQNjY2ePr0KcaOHauKNhIRERH+4UK0qKgoAH8+EAUAzM3N0alTJ2hrayM4\nOBh6enqQyWSqaSURERGVP9L+448/0LFjxxd+lrOMTCZDnz59arRhRERE9KJyQ3vp0qUAgAMHDgD4\n80EoHFkTERG9Pq+8TzsmJgYTJkzAiBEjAAA7duzAjRs3arxhRERE9KJXhvaqVauwdu1axZPLRo4c\niXXr1tV4w4iIiOhFrwxtuVyODh3+9yMCrVq1glz+yovOiYiIqJpVKLQTExMV57N9fX0r9EMfRERE\nVL3KHTJHRkaiU6dOWLJkCT799FPExcWha9euaNasGTZs2KDKNhIRERH+IbQXLFiA3NxcvPXWW5g9\nezb69esHDQ0N/sAHERHRa1JuaHt5eeHRo0cIDAzEpUuXsGnTJpiYmKB///7o378/unfvrsp2EhER\n/ev94xVlTZo0wYQJEzBhwgQAf57P/uWXX/Dzzz8jOjpaJQ0kIiKiP/1jaGdmZiIwMBD+/v4IDQ2F\nqakpevXqhS+++EJV7SMiIqL/V25ov/3228jPz8eoUaMwevRoLF++HFpaWqpsGxERET2n3NCePHky\nAgMD4eHhgfj4eCQkJKBPnz5o0aKFKttHRERE/6/c0J42bRqmTZuG0tJSREREICAgAA4ODkhPT4eF\nhQWfikZERKRir3y0mZqaGlq1aoXk5GSkp6cjMzMT165dU0XbiIiI6DnlhnZQUBACAgIQEBCA+/fv\no3v37ujbty9mzJiB5s2bq7KNREREhH8I7XXr1qF///5YtGgRunbtinr16qmyXURERPQX5Ya2i4uL\nKttBREREr/DKHwwhIiKi2oGhTUREJBEMbSIiIolgaBMREUkEQ5uIiEgiGNpEREQSwdAmIiKSCIY2\nERGRRDC0iYiIJIKhTUREJBEMbSIiIolQeWgHBwejT58+uHjxoqpLExERSZpKQzshIQF79+5F165d\nVVmWiIioTlBpaJuYmGD79u3Q19dXZVkiIqI6odyf5qwJ2traqixHRERUp9RYaDs7O8PZ2fmF1+bP\nn4/+/ftXaj4GBjqQy9WrrV26uprVPq2Jyd+PHFR3narWUFUdLrPK16nty6yyqmMetaFGXatTl/pS\nF+tUVI2Ftr29Pezt7as8n6ys/Gpozf/k5RVUaDpdXc0KT5uWllPjdapSQ1V1uMwqX0cKy6wyTEz0\nqzyP2lCjrtWpS32pi3VeVrc8vOWLiIhIIlQa2j4+Pnjvvffg5+eHLVu24IMPPlBleSIiIklT6YVo\nAwcOxMCBA1VZkoiIqM7g4XEiIiKJYGgTERFJBEObiIhIIhjaREREEsHQJiIikgiGNhERkUQwtImI\niCSCoU1ERCQRDG0iIiKJYGgTERFJBEObiIhIIhjaREREEsHQJiIikgiGNhERkUQwtImIiCSCoU1E\nRCQRDG0iIiKJYGgTERFJBEObiIhIIhjaREREEsHQJiIikgiGNhERkUQwtImIiCSCoU1ERCQRDG0i\nIiKJYGgTERFJBEObiIhIIhjaREREEsHQJiIikgiGNhERkUQwtImIiCSCoU1ERCQRclUWKy4uxrJl\ny5CQkICSkhIsXrwY3bt3V2UTiIiIJEulof3HH39AW1sbR44cQWxsLJYuXYrjx4+rsglERESSpdLQ\nfvvttzF69GgAgKGhIbKzs1VZnoiISNJUGtr16tVT/Pe+ffsUAU5ERESvVmOh7ezsDGdn5xdemz9/\nPvr3749Dhw4hMjISu3bteuV8DAx0IJerV1u7dHU1q31aExP9Gq9T1RqqqsNlVvk6tX2ZVVZ1zKM2\n1KhrdepSX+pinYqqsdC2t7eHvb393153dnaGt7c3du7c+cLIuzxZWfnV2q68vIIKTaerq1nhadPS\ncmq8TlVqqKoOl1nl60hhmVWGiYl+ledRG2rUtTp1qS91sc7L6pZHpYfHExMTcfToURw8eBCampUb\n8RAREf3bqTS0nZ2dkZ2djY8//ljx2q+//goNDQ1VNoOIiEiSVBraCxYswIIFC1RZkoiIqM7gE9GI\niIgkgqFNREQkEQxtIiIiiWBoExERSQRDm4iISCIY2kRERBLB0CYiIpIIhjYREZFEMLSJiIgkgqFN\nREQkEQxtIiIiiWBoExERSQRDm4iISCIY2kRERBLB0CYiIpIIhjYREZFEMLSJiIgkgqFNREQkEQxt\nIiIiiWBoExERSQRDm4iISCIY2kRERBLB0CYiIpIIhjYREZFEMLSJiIgkgqFNREQkEQxtIiIiiWBo\nExERSQRDm4iISCIY2kRERBLB0CYiIpIIhjYREZFEyF93A4hIeeP6t67QdCYm+khLy6nh1hBRTVNp\naGdkZGDJkiUoKChAUVERli5dCmtra1U2gUglGKZEVBNUenjc1dUVY8eOxYEDB7BgwQL8+OOPqixP\nREQkaSodac+aNUvx348ePUKjRo1UWZ6owiNggKNgIqp9VH5OOy0tDXPmzEFeXh727dv3yukNDHQg\nl6tXW31dXc1qn9bERL/G61S1hqrq1PZlVlnVMY/aUKOu1alLfVFVnbrUl7pYp6JqLLSdnZ3h7Oz8\nwmvz589H//79ceLECfj6+mLp0qX47bff/nE+WVn51dquvLyCCk2nq6tZ4WlfNhqr7jpVqaGqOlJY\nZpWhipG2qkbzdalOXeqLqurUpb7UxTovq1ueGgtte3t72Nvbv/BacHAwHj9+jAYNGsDGxgaLFy+u\nqfIkQbx4i4jon6n08PjZs2cRFRWFmTNn4tatW2jSpIkqy5OSGKZERLWDSkP7008/xddff41z586h\nsLAQDg4OqixPREQkaSoNbUNDQ/z888+qLFmn8UpoIqJ/Fz7GlIiISCIY2kRERBLB0CYiIpIIhjYR\nEZFE8Fe+aghvkyIiourGkTYREZFEMLSJiIgkgqFNREQkEQxtIiIiiWBoExERSQRDm4iISCIY2kRE\nRBLB0CYiIpKIf93DVfjQEyIikiqOtImIiCSCoU1ERCQRDG0iIiKJYGgTERFJBEObiIhIIhjaRERE\nEsHQJiIikgiGNhERkUQwtImIiCSCoU1ERCQRDG0iIiKJYGgTERFJBEObiIhIImRCCPG6G0FERESv\nxpE2ERGRRDC0iYiIJIKhTUREJBEMbSIiIolgaBMREUkEQ5uIiEgiGNpEREQSwdAGwFvViYgqht+X\nr9e/OrTv3LmDgoICyGSyGq8l5Q39ZW2vif68zjp1RU31rWy+d+7cwfXr12ukRkhICKKiol5atzqp\nav2/rE5paalKale35ORkODg4AIBKvi9rUnnrXyrfC//a0L516xb279+PzMzMGpl/2QZw7949PH78\nGAUFBTVWoyaVlpYqPqQxMTGIj48H8OcHtzrrCyEUdeLj45GdnV3jdZKTk6ttvi+rA/wZco8ePcKj\nR49qtE5NbmfAn+shKCgIP/74I4yNjat9/oWFhYiIiICRkRFSU1MRFxenqFudnl//9+7dQ0pKSrXO\n/691AgICcOzYMRw4cAAAoKZWvV+5Zev/5s2buH79Ou7cuVOt8y/TuHFjREVFYenSpTUy/zJl/cnK\nykJxcfHfXq+O+Zet//Pnz8Pd3R3h4eEoLi6WzM6IukPZ7tO/SHZ2NhYuXIiGDRvCzs4O9erVq/Ya\nMpkMfn5+WL9+PcLDw5GdnY0mTZpAR0enWmsAgLOzM44cOYLMzEw0bNgQ+vr61V7DyckJu3btQlJS\nEvz9/TFgwADIZLIXQr066jg7O+OXX37Bw4cP4enpiUGDBlXrh6lsXseOHcOhQ4cQHh6OoqIiNG3a\nFOrq6tVaJyAgAOvWrUN8fDwuXboETU1NtGzZstpqlNWp6e2szJkzZ+Dk5IS5c+dCS0sLxcXF1RZC\n6urqMDAwwNGjR7F37174+vri2rVrGDx4MIAXv2yromweBw8exP79+6GhoYEWLVpAQ0Oj2usEBgbi\np59+wvDhw7Fz507k5eWhW7du1Vbn+R2DNWvWQFdXF/v370fbtm3RqFGjautL2Xq2t7fH0aNH4evr\nCzs7uyrP92VkMhn8/f2xfPlyPHjwAElJSejYsaNi572q/Sn7+/379+P8+fPQ1tbG6dOnYWpqimbN\nmlVHF2rcv2akXbanlpubi4YNG2LZsmVITEyEj49PjRyyio+Px9atW7F9+3YYGxvD2dkZXl5eyM3N\nrdY6Tk5O8Pb2xsCBA+Hh4QFnZ2ckJiZWeb6JiYmK0U5QUBDc3d2xb98+6OnpITQ0FMuXLwdQvSOH\n4OBguLm5Ydu2bSguLq6xIwn+/v5wd3fHpk2bcOPGDURGRiq+tKtLWloafvzxRyxbtgyLFi2CnZ0d\njhw5gtDQ0GqtU5Pb2V9H8XPmzMF7772HqVOn4tmzZ5DL5S+MhpTx/N9nZ2cjKioK+/btw4EDB5CR\nkaHYzqpzxy0wMBAXLlzAr7/+isGDByMlJQWXL1+ucp3k5GS4uLjg2bNnAIALFy7gk08+QWlpKZo0\naYJJkybh7t27Va5TWFiomEd6ejp27tyJLVu2oHnz5sjLy8OmTZsQExNTLctMCAG5XI6EhAQUFBTg\nl19+QVFRERYsWPDCNNUlLCwMW7duxcqVK1FYWIgzZ87gyJEjAKq2zJ49e6b4TOTn5yMmJgY7duyA\nhoYG9PT00Lt3bzx+/BhA7T8qtPyOAAAgAElEQVRM/q8YaZeWlkJNTQ0XL17E5s2b4e7ujh49eqBv\n377YtWsX9PX10bJly2oNoKKiIujq6iItLQ1+fn6YPHkyTp8+jcjISCQmJsLa2lqp+T6/tymEwOXL\nlzF58mTcu3cPt27dgpGRER48eIBnz56hcePGSo0eCwoKsHnzZiQlJaFp06YwNTWFgYEBLl26hNjY\nWKxcuRIuLi44ffo0hBBo166dUh+o50fp8fHx0NLSgo6ODm7cuIHbt29j3bp1iImJwYMHD9CkSZNK\nz7/M88vsyZMnuHfvHkxNTZGWloakpCQsW7YMoaGhMDAwqLajLvXq1cP169cxbdo0aGpqwsTEBDk5\nOXjy5AksLCyqpQZQc9sZ8L+jBcuXL0dycjKuXLmCxYsXIzk5GVu2bMHYsWOhqamp9PwLCwvh7e0N\nQ0NDRTiEhYWhS5cu0NPTg52dHXbv3g25XA5zc3Ol6/x1hJadnY3AwEDcvn0bfn5+CAgIwNmzZ6Gj\no4N27dopXSchIQFbtmxBvXr1YGFhgZSUFPj6+sLHxwdr1qyBiYkJDh8+jLZt20JbW1vpOrGxsTh5\n8iR0dXWhq6uLpk2bIjY2FidOnMC2bduQkpKCnTt34vbt2+jWrVuV1pFMJsPly5exfPlyxMbG4saN\nG1i5ciXc3Nxw/vx52NnZVesOVXp6Ot544w3k5ubC29sb48ePh5+fH27fvg0dHR00atSo0vMsLS1F\nbGwsHjx4gLt370JDQwNubm7w9vZGTk4OVq9ejYSEBJw/fx5WVla1/jB5nR5p5+TkIDMzE2pqarh+\n/Tr27t2L1atXo2XLlpg/fz6Ki4vh6OiIn3/+GT4+PtWyhxUeHo6TJ0/C0NAQQ4cOxZ07dzB//nyM\nHTsWVlZWeOONN6p0iLRsg4qLi4NMJsOQIUOQmJiIixcvYt++fbC0tERAQABWrVql1GhLCAFNTU18\n8sknyMzMhIuLCzIyMtC3b19kZ2dj4sSJaNGiBfr374+mTZuic+fOSu3sCCEUfxcQEABXV1c8ffoU\nx48fh4+PD7Zu3Qp1dXV4eXkpzqMr6/lD78eOHUPbtm3x22+/Yc+ePdi6dSvkcjlOnTqFgICAKtUB\n/lz/x48fh1wuh6amJubMmQMA0NfXh56eHmJjY6tco6xOTWxnGRkZius84uLi8N1332Hbtm0wNjZG\nQEAA1q9fj4ULF8La2hpTpkyBEELpz42GhgYaNGiAcePGYeHChdDR0YG+vj5CQ0ORnJyMevXqYcyY\nMdDS0lJq/sCLgX358mXFCPSDDz7A06dPMWvWLHz33Xf4+OOPUVRUpHSNkpISdOzYEY6OjnB3d8ep\nU6fQrFkzJCYmYvTo0WjUqBFu3rwJHx8fpa87yMzMRGhoKDp06ICQkBDMmjULjx8/xqBBg6Cjo4Ou\nXbvC1NQU1tbWePfddzFy5EjUr19fqVplbt26he3bt2P79u1488034eHhge+++w5bt25FWlpatZ3j\njoiIwK1bt2BmZgYrKytcvHgRn3/+OcaNG4eGDRviypUrSi83NTU1aGho4Mcff8TKlSthYmKC2bNn\n4+rVq7C1tYWGhgbCwsIQGBiI/Pz8aulPTaqzI+38/Hz8+uuviI+PR+vWrZGVlQVtbW3k5+cjICAA\n06ZNw8qVK9GxY0cUFhaiW7duaNy4cZVqRkdHw8HBASkpKfj555/x3nvv4erVqzh+/DjatGmDM2fO\nYO7cubCwsKj0+Znnp3d2dsbatWvh5+eHMWPGQFtbG+Hh4bCzs0NcXBw6dOiAxYsXo0GDBpVq//M1\nGjRogE6dOsHHxwfJyclo1qwZ7t+/j4yMDISEhODBgwdYtmyZUhclPX36FMnJyahfvz6io6Mxd+5c\nDB48GEOGDEHjxo3h7e0NmUwGHx8fhISE4N1334WBgUGl6/z1oqOtW7eiV69eeOutt2Bqaor4+Hg8\ne/YM8fHx8Pf3x9SpU6t0PcDz6//gwYPYuXMnLl68iJMnT6KkpAQnT57Eu+++W+VzZzW5nW3YsAF+\nfn7o2rUr6tevD0NDQyQmJuLcuXP48ssvERAQAA8PDwwbNgzjxo2DqalppUcmzx9hUVdXR0BAAJ48\neYLx48fD2NgY586dQ3R0NMLDw3H+/HlMmTJFqfVfVktNTQ1Hjx7FiRMnUFJSgjNnzmDSpEkYOXIk\nsrOz4eLigmPHjmHatGmVrlNYWAi5XA41NTXEx8fD0NAQvXv3xq5du2Bubg5LS0sEBgbC09MTbm5u\nmDdvntJHWgIDA9G0aVOoqanh7t270NHRwe3btzFo0CBkZWUhICAAcXFxcHZ2xmeffQZzc3OlzgM/\n/zdqamrQ1tbGw4cP4e7ujpUrV+L06dMIDg7GkCFD0Lt3b7zxxhtK9adMZGQkFi5cCF9fX2hpaaF9\n+/aIiIiAoaEhMjIykJSUhNWrV6NVq1ZK90NPTw/5+fnIz8+Hnp4eunfvjk6dOmH58uW4f/8+PD09\nsXLlSpiamlapL6pQZ0O7Xr16ikOhKSkpitHHoUOHMHv2bAwaNAgxMTE4d+4cPvvssyofsrx79y4u\nXLiAcePGYe7cuQgPD8eBAwewceNGREVF4cKFC5g2bRqsrKwAVP78zPNXcN+8eRMODg64desWPDw8\nMHDgQLi5ueHChQvw8PDAjBkzlDqM9PxOgaurKx4/fozZs2fDy8sLT548QePGjZGfn4/IyEh8+OGH\nSu/kpKamIigoCB4eHjAzM4OJiQlOnDiBAQMGwNLSEm3atEF6ejoyMzMxb948tG7dutI1nv/A5uXl\nIS8vDzKZDGfOnEHnzp3Rq1cvmJmZwdXVFVlZWfjss8+qdATkr+s/ODgYBw8exK5du6CmpoacnBzY\n2dmhT58+Std4WZ3q2s7Kltdbb72FS5cuITg4GNbW1mjevDmCg4PRu3dvDB48WHG9xJtvvqm4qKqy\nnh/5ampqwtbWFs2aNYOjoyMmTpyIwYMHQwiB5ORkzJ8/X6n1EhISAh0dHejo6OD+/fvYv38/fvnl\nF1y6dAmlpaUYPXo0EhIScP/+fXh7e2PZsmWV3s6ePHkCR0dHmJubIy4uDkuXLsWJEyegr6+Pvn37\n4sCBA7C2tsbIkSMxYMAA9O/fH126dKl0X8qUhdbevXvRpk0bfPrpp/Dx8YG3tzdmzJiBZ8+eobi4\nGEOHDlX6e6bsb8puv2vYsCHatWsHX19f9OrVCzY2NkhNTcX9+/dhY2OD7t27K90f4M+R/M2bNzF9\n+nRMmDABTk5O0NLSglwux5UrV+Dm5oYxY8ZU+vTI85//P/74A66urrC1tcXAgQNx+PBhFBYWYsSI\nEejVqxesrKwwadIkNG/evEp9URWZqO1n3ZVQtnedm5uLixcv4tatW2jSpAlGjhyJw4cPQ0tLC+bm\n5oiOjlYcuqqK8PBwLFu2DPXq1YOOjg4OHjwIAPj6668RGxsLJycnFBcXV/owX9mqKdv4jh49ChcX\nF9SrVw979uyBlpYWHBwcUFJSgk8++QRZWVkwNTWtUn9cXFxw6tQpLFu2DGPHjsV///tfTJo0CRs2\nbECTJk0wePBgmJmZKXXu9/kP0vfff4/du3dj27ZtGDp0KHbs2AFPT0/s2rULTZs2Vbr9f3Xw4EEE\nBQWhdevWGD58OAICAhAREYE5c+agffv2KCoqgpqaWpWuHC9v/S9duhRRUVFwdnaulgvdamo7e15C\nQgLMzMywcuVKAMC8efPg5OSEvLw8WFlZ4cSJE9i4cSMMDAyqdDVv2YWAvXv3RuPGjTFmzBiEhobi\njz/+wODBg2FhYYGePXsq3Y8NGzbA3d0df/zxBxo2bIilS5fCzMwMcXFxWL16NZKSkpCYmAgbGxsU\nFBQodd43IyMDx44dQ2xsLPLz87Fy5Uro6OhgxYoV6NWrFzp06ABHR0eMHTsW06ZNU7ovZcs5ISEB\nBgYGcHZ2RkZGBjp37gxra2ts3rwZaWlpMDQ0hIODA/T09Ko0wo6JicGiRYtgYmICS0tL2Nra4tq1\na8jNzUXz5s1x5coVLFmyBA0bNlSqP8XFxZDL5QgJCcHy5cuhpqaGSZMmYebMmQgPD8fevXthY2MD\nCwsLGBoaKq55UGZbO3v2LA4fPqy4fungwYNQU1PD7t27IZPJYGhoiCVLlkAulyvVl9ehTo60hRBI\nSUnB6tWrYWlpiQYNGihG3GUj8J9++gkTJkxAp06dKj3/1NRUhISEoGXLlggPD8f333+P9evXY86c\nOTh69CiuXLkCOzs72NraIiQkBCYmJjAzM1Oqjp6eHoA/D43dunULc+bMQWBgIGJjY9GvXz8MHDgQ\nnp6eCAkJweTJkyt9ePf5D0Nubi5cXV3x2WefISIiAqWlpXBzc4O2tjY6duyIiIgIDBkyRKnbiZ6v\nc+fOHXTo0AGNGzdGbGwsNDQ0MGHCBOTl5WHVqlUYPny4ot9VERISgiNHjuC///0vunTpgpYtW8LM\nzAxCCOzbtw8dO3aEiYlJpc/JV2b9h4WFwdjYWKlDiKrazoD/HbL+4osv4Onpie+++w4+Pj6IiIiA\nra0toqKiEBERgXHjxik+M8oG9qVLlxATEwMHBwdYWFjg7t27iImJweDBg2FkZITDhw9jwoQJMDQ0\nVLof/fr1Q0ZGBjZt2oRJkyYhKioKe/bswalTpyCXy3HixAlcu3YNNjY2kMvlSvVFR0cHbdq0wePH\nj3H27FkMHDgQzZs3h5WVFTZt2gRzc3NMnDgRBgYGVTr1VnZB4IoVK2BsbIyBAwciMzMTt27dgkwm\nw5QpU5CUlISBAwcqLqSrygj77Nmz+OKLLxSnXVJTU6GpqYn8/Hz4+/tj9OjRSn1vPnz4ENra2pDL\n5YiIiMDly5cxZ84cWFtb4+LFi9DS0kLv3r3RqFEjHD9+HLa2torD1cr0Jzo6Gjt27MCSJUswfPhw\nNGnSBIsXL8bYsWNha2uLrKwsjBo1qkaeO1CT6mRoy2Qy6Onp4cmTJ/D19YW1tTX09PQUoT169GhM\nnjxZqQ0P+PP8qLGxseJ2gY0bN0JTUxN9+vTBpEmTsHfvXnh7e2PUqFEYNmyYUl/Yubm52LlzJ7p2\n7YrCwkLMmDEDpqammDRpEmxsbODk5ITIyEj0798fQ4cOVfSxsso+DAcOHEBQUBCKi4uRk5MDf39/\nbN++HZaWlnB0dERJSQnmzp0LExOTStd4vs7hw4dx6NAhNGjQANOmTcPdu3cREBAAMzMzWFtbIz8/\nH+bm5pU+Hw/8/SrhlJQUREZGYuLEiYr5HTt2DE2bNoWxsTE6dOig1DKrzPofOnSo0uf8VLGdlSnb\nQRw3bhz++OMPXLx4EevXr8f58+cRGxuLjz76CKNHj0a7du2qdD0GAPz8888ICwtD79690a5dO2hp\naSExMRHXrl3DO++8g6lTpyq1nZUdYQP+vANiwIAByMvLw7p16/Ddd98hMzMT+/fvR3R0NIKCgrBw\n4UIYGRkpPSKNi4uDvr4+zMzMIJfL4eHhgRYtWqB169bQ0tJCWFgYxo0bp1Rgp6eno6CgANra2oiP\nj8eyZcvw3XffoV27digtLUWDBg2QkpKiuC1z2rRpaNmyZZXvZQ4LC8O2bdtgYWEBc3NzmJmZISws\nDCUlJejZsydmzpyJ9u3bK1Xn+PHj0NbWhomJCU6ePImzZ8+iX79+6NOnDwoLC+Hp6Qk1NTX069cP\nNjY2MDIyqtT8/9qmJ0+eIDo6GpcvX8aAAQNgZWWFRo0a4aOPPsLw4cMxYsQIpXYMX7c6F9oxMTH4\n8ssvFaPovLw8nD17FtbW1tDQ0MD9+/fRuXPnKu35NmrUCBoaGvj6669Rv359fPnll3B0dERRURG6\ndeuGSZMm4ddff4WVlRWMjY2V+hBpaGigd+/eCA8PR3R0NObNm4eff/4ZWlpa6Nq1KwYPHow9e/bg\n/v37eOutt6Crq1up+f915Ft25fmTJ08QHx+PlJQUjBo1SnFh2/vvv1/l0wghISE4ePAgdu/eDQMD\nA2RnZ8Pa2hqJiYlwd3fH/v37sXbtWqUuBnm+PyEhISgpKUFBQQGKioqQkJCAxo0bQ1tbGz4+Puja\ntStsbGyUHs2rYv2rsk5mZiZWr16Nx48fw8LCAm+//TacnJxw7tw5bNy4EV5eXujYsaNSo57n18uN\nGzeQk5MDe3t7JCYmIiAgANbW1mjdujXq1auHjIwMmJubK3UxYFxcHK5du4bWrVvj0KFD2LVrF9zd\n3fHpp59CJpNhzZo12Lp1K9q0aQM9PT1Mnjy50hc2lSl7oM26desQHh6OwsJC9OrVC3K5HD/++CNK\nS0vh6emJ8ePHK33k48iRI2jWrJniAqrs7GwkJSXh0qVLcHJyQnp6Opo2bQp9fX2Ym5srAk7ZHZDw\n8HDExMSgU6dO6NmzJzZu3AgrKyu0a9cOzZs3x/Xr19G9e3fFzpQy21qXLl1QUlKCDz/8EOvWrUNO\nTg7OnTuHLl26oHv37sjLy4Onpyd69+5d6UPvz29n7u7uCAoKgoaGBnr06IHc3FxcuHABPXr0gIWF\nBdq0aYOmTZsqfXHj61anQvvu3bto164dzp8/j5MnT2Ls2LEwNzdHWlqa4slEQ4YMUfqe37IN49Gj\nRygsLESjRo1w5swZGBsbY+7cuVizZg2ePHmCnj17YvLkyTAxManSlZulpaXIyMjADz/8gDfffBPv\nvfceVqxYAR0dHXTu3BnDhw9H+/btq3RI3NnZGaGhoWjVqhVmzpwJXV1d3L9/H76+vggKCsKVK1fw\nySefKBXYf93zvXPnDpKSkpCWlgZvb28cPnwYN2/exNSpU9GvXz+MGzeu0nvXf63l7OyM3377TXFf\ncVFREbS0tODm5obk5GS4ublhwoQJSt0Ko4r1r6o6ZTWys7NRXFwMY2Nj+Pr6Ii8vD+bm5hg3bhwc\nHR3x8OFDrFy5Uun18vwRlr179+LOnTu4cOECvv32W4SGhsLb2xuWlpZo27YtrKyslNqRevr0Kdzd\n3REVFYV79+4hKCgI3377LYqLi/HNN99g2bJlUFdXxxdffKG4elvZ87HAn88UWLNmDXbs2IGYmBic\nPXsWhoaG6N+/P4qLi+Hv74/PP/+8Sufky46wOTo6wtraGo8fP0ZycjJGjhyJ2bNnIzc3F2lpaZgx\nY4bS6wb43+NpV69ejfr162PlypUYNWoUrKyssGbNGpibm6NDhw7o1q2b0oeRy7a1x48fw9jYGGFh\nYdi/fz9Wr16Nu3fvws3NDZaWlujVq9cLOwaV7Qfw5xFDHx8fmJmZ4cSJE2jVqhV69+6N+Ph4uLq6\nom/fvujYsaNkAxuoA6FdtkHcv38fjo6OuHbtGjZt2gQ/Pz8cPnwY48aNg4mJCcLCwmBnZ6f0FYJl\ndS5fvowVK1agUaNGivOyLi4uMDIywuzZs+Hg4ABbW1vo6ekpdf/y82EaFBSEpk2bYtCgQdiyZQta\ntWqFDz74AF988QVMTExgZWWl1KikrMaFCxdw9OhRNGvWDAEBAdDS0sKAAQNgZGSE+Ph46OvrY+XK\nlUrdovR8YIeHhyMtLQ09evRAbGwsnjx5Ant7e3z88ceIiIhAZmYm+vXrp9QXdnh4OBo1agSZTIbY\n2Fjs2LEDO3bsQM+ePaGrq4vQ0FBYWFigWbNmiI+Px8KFC5XeBlSx/lVVRyaTwdfXFw4ODsjIyECz\nZs3QqlUrnD17FiUlJdDR0UFJSQlsbW2rfGHgtWvX4OTkhP379+PBgwc4c+YMwsPDsXr1apw+fRoJ\nCQno2bOnUhc3xsbGwsfHB2PGjEFqaiqio6Ohp6eHt99+W3F0bfXq1di0aRPy8/PRsmVLpU69PK+o\nqEhxePry5cuwt7fHmTNncPfuXchkMixatAht27atUo2IiAi0atVKcf7f3t4eI0aMgJaWFq5du4ZD\nhw7Bzs5OqZF8eno67t69CyMjIzx9+hSbN2/G7NmzMWnSJHTs2BGrVq3CmDFjYG1tDQcHB4wfPx7a\n2tpV2gH18fHBrl27kJOTg/nz5yM4OBi//vor1q9fj6ioKJw+fRq2traVXjeRkZHYt28f+vbtCwBw\ndXXFhg0bEBkZibS0NMybNw/FxcVo0aIFkpOT0bFjx0oflax1RB3g6+srPvroI7F9+3bxwQcfiOXL\nlwshhPjss8/E5MmTxZgxY0RgYKBS8y4tLVX8961bt4S9vb24f/++yMnJESkpKeLy5csiKChIfPbZ\nZ+LixYuioKBAqToZGRmK/z516pSYNWuWuHPnjkhNTRVCCBEdHS1mzpwpzpw5IxISEkR8fLxSdcrE\nxMSI+fPnC3d3dyGEEOfOnRNfffWVOH36tBBCiBs3brzQJmUdOnRIvPvuu+Krr74SixcvVrx+48YN\n4enpKaZPny4SEhKUnv/BgwdFcnKyEEKInJwc8dVXXyney8jIELt27RKenp5CCCGKioqUriNEza5/\nVddJSkoSM2fOFGFhYeLJkyeK1y9fvixmzpwphg4dqvjMPP8ZqIi/Tp+XlydcXV3Fzp07xeeffy6K\ni4vFhAkTxKxZs8RPP/0kHjx4UKW+ZGRkiLt374o7d+6II0eOiC+//FJ4eHiI4uJiIYQQS5YsEXfu\n3FF6/mX9CQ8PF05OTuLhw4fi7t27Yvv27eL69etCCCE2btwodu/eLS5dulSlvpTVmjt3rpg+fboQ\nQoj169eLJUuWiISEBHHs2DHx7bffiosXLypd49y5c+L+/fsiNzdXCCHErl27xNGjR0VeXp4QQohL\nly6JDz/8UAghxMOHD5WqkZCQIOLi4oQQQkRFRYkpU6aIhw8fiocPHyrWy/fffy/efvttUVJSovT3\nWUJCgpg+fbrYsmWLEEKIBQsWiHfeeUcsXbpUFBcXi+TkZLFjxw4hRNU//7WFpEfa4v/34vbs2YNu\n3brho48+Qs+ePREcHIzAwECsW7cOzZs3x/Dhw5W6pzQpKQlhYWFo0aIFgD+fX3v79m08ePAA/v7+\nOH36NGJjY/H06VMMHjwYxsbGSh16T0lJwdmzZ9G2bVvI5XLs3bsXtra2MDIywtmzZ7F3717FobDf\nfvsNEyZMUPqCsDJlt5BcvXoVrVu3Rs+ePSGTyeDq6gq5XI6+fftW6VGLwN9HWB4eHvDz88Pbb7+N\nU6dO4c6dO5g3b55S5xbv3buHVatW4eOPP0Zubi6mT5+OmTNn4vTp04rHK2prayMwMBAZGRno1asX\nZDJZlZ9fXBPr/3XUKS0thZ+fH3r37q04knLu3Dnk5eVh7ty5sLGxqfIzBU6ePImgoCAkJyfDzs4O\ngYGBsLCwUDwTQQiByZMnKzWSF8/dDqmtrY3Vq1cjOjoaY8aMQUFBAW7fvo3o6GhkZ2fj9OnTmDBh\ngtIPzin7EYv169crjkD06tUL7u7uOHv2LFq2bIkzZ85gzpw5Sj3Q5nnJycnQ19fHqFGj4OPjg1On\nTmHDhg0ICwvDpUuXMHHiRNjZ2aFt27ZKXxDYunVrqKurY9WqVVBXV0dJSQkePnyIevXqoXnz5igq\nKsK1a9cwZMgQ1K9fX6m+ODk5QUdHB82bN0dkZCSio6Nhbm6O06dP4+DBg9i7dy82btyI27dvw8DA\nAB06dKjU/Mv60qBBA/Ts2RPHjh1DamoqPvroI5w6dQo9evRAjx49cOnSJcVvM1T1+6y2kHRoFxUV\nQV1dHQ8fPkRubq7iXIW2tjZOnTqFyMhIvPPOO3jjjTeU+iC5ubkpbm969OgRioqKYGpqitDQUEya\nNAnvvfcemjVrhsePH2PUqFFKXUCVlJSE5ORkDBgwAPfv30d0dDQsLS2xc+dOREREwMrKClOmTIGv\nry9sbW0xYcKEatn4dHR0YG5ujoyMDFy9ehVNmjRBjx49oKurC0tLS6UOVf91GTdo0AByuVxx69Ch\nQ4dw+PBhuLq6wszMDB999JFSVzyL/39OdUxMDDw8PPDOO+8oHh+6Y8cOnDp1CufPn8eDBw9w5coV\nzJkzBw0bNqzSfaupqak1sv5VVaesRlRUFDIyMqClpYVnz54hLi4OOjo6MDY2VvziWefOnat8Va2H\nhwcOHz6MgQMHwsTEBM2bN8eDBw9w69YtxZO7lixZotR50rK+yGQyREZG4tmzZ3jnnXcQHByMsLAw\njBo1Ck+fPsXFixeRl5eHb775pspPoTt79iwGDRqEGTNmKG6rsrCwQEhICAICAjB16tQqPdAE+POZ\n6Bs2bMDDhw/RuXNnjBgxAp6ennB2dsbGjRtx5coVvPnmm4qLaJW9rSsxMREGBga4d+8e4uLiFNf9\nxMbG4syZM/Dw8MCUKVPQtm1bpftiaGiIgoIChIeHo1+/frhx4wa8vb0xbNgwzJ8/H48fP0Z6ejo+\n/vjjSu+APv89k5KSgjfeeAPdu3eHs7MzHj16hK+//hpbtmxBSEgILly4gNWrV1fLznRtIdnQDgoK\nwqlTp5CZmYnmzZvDzc0NxcXFePPNN/HkyRMUFxcjLy8PpaWlSm98lpaWuHr1Kn799Vekp6fj+vXr\n6NevH6ZOnQo9PT14e3tj9+7dGDJkiGI0XhkFBQWKW1GMjY0RGhqK69evw8zMDF988QXGjh2Ltm3b\nIjY2FhcuXMDo0aOr9XyMlpYWWrZsiaSkJFy8eBEtW7ZE165dqxzYx44dw9WrVxX3XV65cgUdO3aE\npaUlSkpKoK6uDnt7+yqFj6amJkpKSuDv74/Q0FA4OjoiODgYR48eVfzIRGlpqeJWGGWUnffdsWMH\nMjIycO3atWpd/6qsU1bj+++/x61bt5CamgojIyM8fvwYPj4+iIuLw8mTJzF8+HClzvmXrZey/z9z\n5gy6du2KcePGKXbMfH190b59e0RFRWH27NlKf5GWbWdHjhzBwYMHcevWLRw+fBhr166Fl5cX7ty5\ng6FDhyquElemzl93QG/evAl/f3/FT1JGRUXhwIEDcHR0xKBBg6r0QyPAn1e/BwUFoUuXLrhy5Yri\nKY6jRo3C1q1bER0djcDJt4sAABxlSURBVFWrVlXpnuKyIwZr165FZGQkJk+ejMzMTISHh6NXr17o\n0qUL2rZti4EDByp9EV3ZcjMwMMD169cRGBgIdXV1fPjhh3j77bdRv3593Lt3DwcOHMDIkSOr9OTG\n48eP4+eff0ZCQgIePnyIGTNm4OjRoygsLMSGDRvQvXt3jBkzRjI/uVlRkgzta9euwdHREdOnT8e3\n334LKysr2NrawtXVFcHBwfjll1+waNEipKWlQSaTKf2I0uzsbOzZswc//PADUlNTcfPmTdjb2yM7\nOxthYWFwdXXF+++/j/79+1d63mWjxbZt28Lf3x85OTmKe1avXr0KANDU1MS2bdvg4uICBweHGtlb\n1NbWhpmZGTIzM2Fpaan07zCXfZBOnDgBLy8vjBs3TnFLyqNHj3Dnzh1cvnwZcXFxWLx4cZXv9z56\n9CjOnDmDMWPG4O7duzh//jzWrl2L0NBQbN26Fd988w06depUpatEc3NzsWXLFmzZsqXa178q6hQV\nFSmePpWSkoKNGzdiy5YtePLkCZydndG2bVt069YNHTp0wKNHjzBlyhSlv6zL1sujR48Uh6FDQ0Nh\nbGwMU1NTFBYWws/PDzNmzMDQoUOrNML+v/bONS6qcu3/3+EoB5GDKYg0A4iAEmghBigIKAR5ztQd\n1s6yPJTbrF1aokXWbmcH25GSpmZaunWDoB9ASBMDVA4iykmUs4AaEKcRQUZY/xf+ZzaPz7OfJ2YA\nodb3FS+Gudc991rrug/X9fvBvdLOH374gd27d1NXV0dFRQVz585VvQeqq6t5/vnn1S7rk0gkpKSk\nEBcXR2ZmJsuWLVMJ2kyfPp1ff/2VpKQkPD091c5EV/bnwoUL7Nu3j6KiIqRSKRMmTCA1NZXGxkaG\nDh2KQqEgJCREozr8rq4uGhsbeffdd1m/fj1+fn5YW1vj5ubG5cuXyc7OZvTo0SpdfnWRSCTk5+ez\nZcsWXn31Vdrb28nIyKC1tRVTU1N++ukndu3axYoVKzTKrj916hRHjhzh448/Jj4+HkEQCA4Oxt3d\nne3bt3Pz5k2mTZvWJ77yD5pBJ2Oam5tLWloazs7O2NnZERYWRkREBDo6OgiCoFIKunv3LgcOHOD9\n99/vkaaw8kGqrq6mtraWo0eP4uzszOnTp1WlI1euXMHPz4+2tja1tqrvn8XX1dURERGhcriprKyk\nvLycsWPH4uXlRWdnZ59v7yhXwD1F2Zeuri46Ozt56623mDFjBvb29pw9e5bKykoaGhoIDAzk559/\n5qWXXtJoVaJsLywsjMcff5yZM2fS2NjIV199RUdHB5s3b+bvf/87oaGhGq0Ya2pq+OWXXzh69Cjj\nxo0jOTm518a/v9opKyujurpapfpWWFioUgN76aWX2LVrF8bGxlhbW7Nu3TqNsoPhXlnX/v37CQoK\nwsLCgtbWVuDeNnJ7ezt79+7l66+/1ngnJzc3lzFjxhAZGUlzczNyuZxPP/2US5cu8csvvxAcHExt\nba1Gwef8+fNs3bqVDz74gBUrVjB//nxefvllVq5ciZmZGVeuXGHNmjX4+vqq3Yaynffee4/XXnuN\nM2fOMGTIEMaOHcuoUaPYtWuXSnbV09NTY/MPQNUf5aTp7Nmz6OnpUV5erhJU6Q2URzofffQR8fHx\n5Ofn4+zsTEhICA0NDT0em/v7ce7cOVpaWmhtbSU5OZmtW7dy9epVLCws6OzsBNDYyGSgMqhW2iUl\nJYSHh/PYY4/x5ZdfkpqaSmRkJKamprzzzjs4OTmpAvSJEyd45ZVXelx6oSy3CQsL48qVK8THx5Ob\nm8tHH32Evb29KjHI19dX7TKI7mVd3Y054uLiaG9vx8nJCYVCQWVlJT4+Pv1SU6hO2VB3BarW1lYM\nDAzo6uri4MGDZGZm4uTkhL+/Pw0NDcyfP5+QkBCNV1i3b99WieQ0NDSo1NOGDh1KVFQUxcXFbNiw\nQe2ynv4Y//5qRy6Xs2vXLpVmgYuLC3l5eXh4eODn54dcLsfOzk5lDKJuP+BeIM3MzOSVV17h1q1b\nNDU1oaOjw6hRo0hMTKSwsJD169erLWqkbCcjI4OdO3fi4+PD+fPnKSkp4c0338TCwoLU1FSuX7+u\nys1Qh87OTrS0tIiNjWXy5Mno6+tTUlLCqlWraGxsJDQ0lEmTJhEUFKT2Dl73+/nSpUuMGDGCRYsW\n4ezsTGNjIxcuXEAmk7FixQqmT5+ukWysRCIhJyeHpKQkLCwsSEtLIz09HV9fX7S1tUlOTqakpISl\nS5dqnNxaWlpKXl4eMpmM+fPnc/jwYdLS0li1apXqeNHV1bXH7XT/vS5evEhTUxMdHR2sX7+e+vp6\ndu7ciZaWFpGRkRgaGqpdCjtYGDRBu6Ojg2PHjlFYWKjyiTUwMGDy5MnU1dWpXFxMTU0xNzfH29tb\nrQBRWlrKN998w4cffkhoaChVVVVcunSJoqIi7ty5w3fffcfKlSuxs7PTKBM5NjaWY8eOsXz5cpYv\nX87w4cNZsWIFSUlJyOVyxo8fT3Bw8ICuKVT2PzY2lu+++47GxkZMTU1ZtmwZTz31FC4uLqrkFmWN\nqSbtREdHq7Y+nZycSExMpKWlhYcffpiysjKMjY157rnnNNIt76/x78t2umfWZmRkoKOjg5mZGVKp\nlNLSUj766COkUimHDh1iyZIluLq6aiRNWl5ezoYNG7C3t2fu3LmMHDmSmzdv0tjYiJWVFatWrcLH\nx0etgH1/0tGWLVuwsrLiySefZOTIkdTU1KiSnFJTU3n55ZfVSqJTtnPr1i309fVRKBTEx8eTlJTE\nJ598wsiRI9mxYweWlpZYWVlpdI9JJPc0vjMzM7G2tuaLL77A29ubhx9+GJlMRmxsLLW1tUgkEiZM\nmPDffoeecPHiRbZs2YJCoSAtLQ1/f3/S09PJyMigoqKCH3/8kQULFmh87ltfX09aWhqZmZno6Ogg\nlUqZNWsWe/fuJSsri1dffVWl9d9TlP3et28fhw8f5uLFiyxcuJAxY8YQGxuLvb09qampnD9/ngUL\nFmhchz/QGTRBW1tbG1tbW9rb2ykqKsLW1pbhw4fz4Ycfcu7cOZ577jkeffRR1efVWTl2dHRw9OhR\n0tLSmDx5MtbW1vj6+nL16lVqa2uZPn06gYGBTJ48ucff3f2h+9+MOZydncnPz2fOnDkDdrZ46dIl\ndu3ahY+PD4mJiURHR/OXv/yF3bt3Y2JigqenJ5cvX2bPnj3ExMT0SvZmQkICMTExvPHGG6xbtw4v\nLy+efPJJTp48yZkzZ4iLi2Pt2rUavXz6cvz7qx3lfXb+/Hni4uKYMWMGlpaWZGdn09zczIIFC+js\n7CQ/P59FixaprCLVLeuKjo6mrq6O9PR0JBIJI0aMUG3vVlZWUllZyaOPPqqxycyFCxewsLBAR0eH\n8vJyBEHAw8NDJTyjdLrTRJo0LS2NTZs2UVtby/Xr17l79y6TJk3CysqK2tpa9u7dS3BwsMZn2IDK\nw/n5559HEAQ2b96Mh4cHcrmcrKwsXFxcVJrf6k4Qi4uLee+99wgLCyM0NJSOjg6Ki4tV8qq6urrM\nmDFD7ftZ2Z/i4mKSkpKwtrbGxMSEnJwcurq6kMlkGBgYkJCQgLu7e4+FYO5/Z+7evZtPPvmEuXPn\nYmBgwJgxYzA3Nyc/P5+qqirWrl2r9vgPJgZN0IZ7SVP29vZcv36da9euERAQwNKlS/Hz89O4PhLu\nTQzs7Oy4c+cOBQUFGBsbM3r0aPT19Rk2bBh/+tOf1FaH6i6z91uMOQay88yQIUPYvn07NTU1jBo1\nipCQEEpLS6murmbDhg1cu3YNExMT7O3tmTlzploZz93HUqFQkJKSwqxZs6ipqaGlpYXly5dz+/Zt\nZs2ahZeXl8aJOtC3499f7UgkEtLT0/n8888ZPXo0w4YNY/r06bS2tlJUVERqaipGRka88MILPa6N\nvZ/Y2Fji4+NZtGgR//rXvzAyMiIzM5NRo0bh4OCAVCrVaKu6e9Lh3r17KS4u5umnn0ZfX5/k5GQA\n7OzskMlkuLm5abTCKi0tJS4ujtmzZ6OlpUVTUxNyuZyhQ4eqav9XrlypWvmq25+srCzq6uqoq6tD\noVDg6enJhAkTMDMzY//+/Zw8eZIXXniBOXPm0NzcjIeHh9rJVLdv3yY2Npbq6mqCgoJwdHSktLSU\n48ePs2LFCpydndWe5CqPxlJSUvjmm284d+4cxsbGSKVSDAwMyMjIoKCggIsXLxIeHt5jr3L49/gX\nFRVhZmZGUlIS48ePZ8SIESgUCg4ePMiIESN45pln8Pf3H5TmH+owqII2/Dvbubi4mMzMTDw9PVUP\nqyYBW8mQIUMYM2YMVVVVHDp0iObmZo4ePUpwcLBapUPdg09/GXP0FcqcRQMDA0aPHs23337LiRMn\nSExMRC6XExERgUQi4eOPP8bZ2RkHBwe1NL7hv05yfv75ZwoKCjhw4AB1dXVs27YNbW1t3nvvPRwc\nHLCysuq1Y4TeHv8H0c6ZM2ewt7dn2bJlqgmTIAjo6+urVr6aJBwJgkBnZyeHDh3i2WefpbS0lPb2\ndiIjI7lw4QKJiYk8/PDD2Nvbq+VTrUSpvR8REcGnn36qOv5ycHDg5s2bJCcnY2xsrPq91H3+6+rq\nVHXJzz//PDY2NkgkEhoaGpg2bRoLFy7E29ubRx55RK3vV74D5HI5sbGxNDc3U1BQwPHjxyksLOTu\n3bvY2toSHBzMvHnzVImaDg4OPdqGV7aTnZ1NYWEhWlpaLFq0iJMnT5Kbm8vUqVMxMTHh+PHjTJ06\nVa1n5vr169TX12Nubk5tbS0bN27kb3/7G0FBQVy8eBGFQoFUKkUqlZKSksLs2bN7PNFR9kOhUNDY\n2MiqVat44okn0NbWZtOmTfj4+Kh0BUpKSlTCUL3x/h8MDLqgDf9ecY8fP75PVqTK76+rq6OgoICZ\nM2cSFBSk0blfXxpz9BfKB+PAgQOkpaURGBjIzZs3KS4uxtLSEh8fH06cOEFWVhYhISFqbe/fP8nZ\nt2+fyjDh8uXLuLm54ejoSGZmJunp6cybN6/Xyzp6a/z7qx3l5zs6OtDW1qampoaMjAxmzJgB3DsL\n3r9/P0uWLCEgIACZhhaOEokELS0tfv31V77//nvKy8uJiIgA4OrVq4wdO5YJEyZoPP5dXV0YGxuT\nnZ3No48+qsoMTkxMRCaT4eTkhJOTE0OHDlX73L++vp6HHnoImUzG/v37eeSRR5DJZMhkMmJiYrCx\nsdFo8gn/rpH/8MMP0dXVRS6XExYWRmVlJa2trUilUnbu3Imfn99/ef57esSnLFPbsWMH5ubmxMbG\n8tBDD/HUU0+xY8cOjh07RllZGUuWLFF7l+X8+fMYGhpiYmLCkCFDiIqK4qmnnsLa2hpLS0v++c9/\n0traire3N0uWLMHW1lblcd6TfsA9IxhTU1MEQcDMzAxfX1+0tLT45JNPqKmpITExkddffx1zc/M/\nTMCGQRq04d4Lry8TDoYMGaISaikoKMDS0rLHSRTKG6kvjTn6E0EQaG9vZ9u2bSxYsICZM2fi7+9P\nbm4u6enpyOVy8vPzefPNNzXeEldOcmQyGUuXLmXkyJGUlZVx4cIFMjMzqa2tZc2aNWpbH/5f9Mb4\n91c7yqAQFRXFmTNnmDlzJgkJCaSnp+Pq6srNmzc5duwYjz/+eK/uSpmampKRkYG/vz9WVlakpqYS\nHx/PunXr1N6qVF7XsWPHOHz4MI6OjtTU1PDDDz/g4+ODkZERp06dorS0lIULF2pUh52amsqmTZtI\nSEjgiSeewN3dnbCwMExMTOjq6iImJgZ/f3+Nj11KSkr46quv+Mc//kFHRwcJCQmEhoZiZWXF3bt3\nWbx4MYGBgRrdX8rdj+3bt7Nx40Zu375NXl4ey5Ytw9DQkICAAHJycjA0NOS5555T/U9P7wNbW1uK\niopYsWIFAQEBGBkZsXXrVry9vZFKpXR0dJCXl4dcLkdHRwdLS8vf3Eb368nLy+Ott96iuLiYlJQU\nysvL8fT0xMPDA3d3dxwdHZk/f36fPf8DmUEbtPsDAwMDbGxsaGhowM3NTa0V3ZUrV9i/fz/z589n\nyZIlDBs2jFOnTnHnzh2mTJmCra0tc+bM0cjfu7+QSCTo6upSV1dHc3Mzo0aNUolnZGdn4+bmxrvv\nvquxcMr9kxw9PT2mTp2KnZ0dHR0d2Nra8te//rXPa9d7Y/z7o53c3Fy2bt3K22+/zbZt27h9+zab\nNm3i7NmzZGVlERMTw8qVK9Xe3v1PKL2cMzIyOHDgAPn5+bzzzjtq3ctlZWW0tLSoBDhiYmKwsLBg\n165dqpKhI0eOkJOTo8pGVichTLnqa2hoICoqiuXLl+Pg4MDbb7/NwoULmTRpEps3b6a1tZW33nqL\n8ePHa7zDcufOHW7cuEFVVRXJycl89tln1NTUEBUVxblz55gzZw46Ojpq6SQoUSgU6OrqcubMGQoL\nC8nOzmbDhg2YmJhw5MgRpkyZgpubGwcPHuTGjRsqLf6eUlpaSnh4ONOmTeP7779n4cKFDB8+nLCw\nMIyMjDh8+DCLFi2ipKSElpYWXFxcfnO/uic3NjU10dbWxtq1a7lx4wYnTpygq6uLL774AhMTE6ZN\nm6bR7segpscWI39AlK406lBfXy98/vnnwurVq4WioiJBEO657KxcuVLlPjXYuHbtmhAWFiYcPXpU\nkMvlwunTp4XIyEiNnccE4T+7j8XFxQmCcM9pqa6uTuN2eoIm49+X7XR2dgqCIAiHDh0S9uzZI1y6\ndEl48cUXhZs3bwpXr15Vfaavf6+Ojg6hvr5ebVe4jo4OIT4+XpDL5UJKSoqwZMkSoaysTBAEQfj2\n22+FP//5z0JJSYmQl5cnZGVlqeUK197ervr79OnTwqpVq4Tg4GChqqpKEARBSEhIEAIDA4Xc3Fwh\nPT1dCAwMVD2vPXU5u59bt24J4eHhwoIFC4SCggJBEAThxx9/FI4ePaqR+5iSzMxM4f333xdSUlKE\n+Ph4wcXFRTh58qQgCIJw7tw54ZlnnhHq6+sFQbjn7qauc9edO3eE3bt3C3PmzBEuX74sxMXFCQsW\nLBCuXbsmnD59WoiIiFC5nF27dk3VZk+IiYkRXnjhBaG8vFxwcXERUlJShMbGRmHjxo1CZWWlkJeX\np7Er3GBHXGn/BtT1RYa+MeZ40AwbNgypVMqpU6eIjY3l5MmTvPbaa2oLdHRHIvnf3ce8vLz6XZpQ\nk/Hvi3aE/7/ya2xsxMDAAD09PZWs65dffsmIESP4+uuvMTQ0xNraWiMRmN+CtrY2hoaGaqvDaWtr\nY29vz40bN4iMjCQ7O1ulojdhwgRaWlr4/PPPCQgIYOLEiT0+FmtubiY2NhZXV1cKCgqIjo4mNDSU\nu3fv8vXXX+Pn58fEiRMxNTUlLCyMsLAwFAoFkZGRzJ07Fx0dHbX6pURPTw+ZTEZFRQX19fUUFRXx\n3XffERgYiJubm0bffenSJTZu3MicOXO4desWs2bNwsbGhi+++ILq6moOHz7MmjVrcHR0BO7tjqhb\nSqqtrY1MJqOtrY3s7Gy8vLywtbUlPDycefPmERISglQqpaurC1NT0x49p8J9yY3Ksr6lS5eSn59P\nSUkJjz32mMb5Bb8HxKDdD/SmMcdAwdzcnMceewxXV1dmz57dK6VQ8Puc5PQ2ypri9evXU15ezuXL\nl7Gzs0MqlaKjo8OdO3fYt28fs2bNwsLCYlAk6UgkEjo7O5HL5Zibm5Obm0tFRQU+Pj64ubmhra3N\n2LFj1XphKxQKbGxsaGxsJCwsDLlczurVq5k6dSq1tbVs376dqVOn4u7uTkhICMbGxkycOLFXxY1M\nTU0ZN24czc3NXL16laVLl+Lp6anWdwndtusrKiowNzdn8eLF2Nvbo62tjZGREV5eXjg6OuLv74+7\nu3uvJVEqkyerqqrIyclh0qRJODg4oK+vrzr7V1e5rXtyY1lZGV999RXa2tpER0djYGCAk5PT7144\n5bcgBu1+oreMOQYSenp6mJmZ9boIzO9xktOblJaWcuTIEZVNZGFhIdeuXcPDw4MjR45w9uxZXnzx\nRbU85B8khoaGyGQyfv31VwBycnIoKCggICAAFxcXtVdY+vr6GBgYEB0dTUtLC/n5+fzyyy/4+Pjw\n+OOPU1lZSUREBE8//bTqHpNIJOjp6fXqhEeZA+Dj46NR0qlSPCcjIwNLS0s+++wzlQJkV1cX77zz\nDr6+vjg7O6s0vnu76kEmk1FSUkJOTg6LFy/utXLI+5Mb09LSSExM5N133x3Q2hX9iRi0+xFDQ0Nc\nXFzE4PMb+D1OcnqDlpYW1qxZg6GhIc8++yyjRo1CKpVSVFSEq6srL774IlOmTGHcuHEP+lLVQjnu\nTU1NtLS0qESUNB1/LS0tpFIpbW1t6Ovrk5WVRU1NDVOmTGHKlCl4eXkxfPhwtLS0VAGuL3coNDVn\nUSqqPfvss3R2drJ582bc3Nyor68nJSUFX1/fPhUbUa641ZUm/U/0ZnLj7xUxaPcz/XU++ntAnOTc\nQ/myvnr1qkp1KiUlBXNzc6RSKcOHDyczMxNDQ0PGjRs36Cc4ymx6hULB6tWrNXLr6o6hoaFqm1xH\nR4effvqJmpoafH19MTU17bdjBE3q4/8nRbWJEyfy0EMPER8fT0ZGBosXL+6XXZa+Krs1MzPD3d0d\nHx8fAgMDB3wpbH+jWYaFiEgfo0kZzO8FiUTC6dOn2b17Nx4eHqxevZq2tjb27t1LRUUFjo6OXLx4\nUWObyIGEhYUFixcv7vXxNzc3Z/bs2XR0dKBQKAgKCgL6dlWtKUI3RbW0tDTMzMyoqKjg1KlTXL9+\nHT8/P2xsbFi/fj1GRkbo6en1uhBQf6Orq4uFhcWDvowByaDz0xYR+aNRV1fHG2+8wccff4yenh5N\nTU10dXVRXFzMvn37sLOzY/Hixbi6uj7oSx00NDQ0cPv2bUaPHj0oAtzPP//Mnj17sLGxQVdXlw0b\nNvD+++/T1NREQEAAUVFRfPDBB2qJGokMLsSVtojIAEQZSBoaGqiursbY2JiffvqJ9PR0DA0Nqa6u\nZu3atbz++uts27aNxsZGOjs7xZ2J34i5ubnqzHegB2ylZ8GXX35JcnIye/bsQUdHh2eeeUYlzhIQ\nEPCHP0b6oyAesIqIDECU0qSvvfYaCQkJpKSkIAgCK1euZMuWLSxbtoyYmBg8PDyYN28eBw8epL29\n/UFftkgfYGRkhLOzM7GxsRw/fpwdO3ZQVFREVFQUSUlJKBQKjYxZRAYXYtAWERmAdF9dPfLII7i4\nuBAaGqoK5jt27GDWrFkAzJ07l08//bTXaopFBhYmJia0tbURFxfHmjVrsLKyoqqqCldXV5UJia6u\n7oO+TJF+QjzTFhEZgNy4cYPvv/+eESNGcObMGcLDw2lra+Obb77BzMwMb29vvL29Vb7Gg+FcVkR9\nKisr2blzJ1ZWVpiYmBAdHc26devw8vJ60Jcm0s+IQVtEZADS2trKZ599Rl5eHuHh4YwbN44TJ07Q\n1NRESEiIuKr+A3Ljxg3OnTtHfn4+QUFBTJ48+UFfksgDQAzaIiIDFHF1JfI/Ie6q/LERg7aIyABG\nXF2J3I8YtP/YiEFbRGQQIL6oRUREQMweFxERERERGTSIQVtEZBAgrrJFRERADNoiIiIiIiKDBjFo\ni4iIiIiIDBLEoC0iIiIiIjJIEIO2iIiIiIjIIEEM2iIiIiIiIoMEMWiLiIiIiIgMEv4fjLrWYkDr\n/nkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7febcc7c3ba8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = classifier.get_variable_value('linear/linear_model/x/weights').flatten()\n",
    "sorted_indexes = np.argsort(weights)\n",
    "extremes = np.concatenate((sorted_indexes[-8:], sorted_indexes[:8]))\n",
    "extreme_weights = sorted([(weights[i], word_inverted_index[i - index_offset]) for i in extremes])\n",
    "\n",
    "y_pos = np.arange(len(extreme_weights))\n",
    "plt.bar(y_pos, [pair[0] for pair in extreme_weights], align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, [pair[1] for pair in extreme_weights], rotation=45, ha='right')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Most significant tokens') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fIXCY2YCl6z-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Vamos Deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsCh3Dzccu7L",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fully connected\n",
    "\n",
    "Acá podemos bifurcar los caminos, siguiendo complejizando los modelos lineales, uno de los grandes campeones de la clasificación de texto son los SVMs, con kernels no lineales, pero si queremos ir deeper, podemos agregar algunas capas intermedias..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5648
    },
    "colab_type": "code",
    "id": "86jMfEf4NpLW",
    "outputId": "b91a37f6-c235-4703-c397-d0900284d4d6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp3isqf8hw/bow_embeddings', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f21d4fd57b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp3isqf8hw/bow_embeddings/model.ckpt.\n",
      "INFO:tensorflow:loss = 68.389984, step = 0\n",
      "INFO:tensorflow:global_step/sec: 49.9202\n",
      "INFO:tensorflow:loss = 60.148384, step = 100 (2.008 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.7012\n",
      "INFO:tensorflow:loss = 59.02127, step = 200 (1.645 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.7991\n",
      "INFO:tensorflow:loss = 40.57781, step = 300 (1.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.1766\n",
      "INFO:tensorflow:loss = 31.678307, step = 400 (1.638 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.1833\n",
      "INFO:tensorflow:loss = 33.37588, step = 500 (1.608 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.5105\n",
      "INFO:tensorflow:loss = 19.373632, step = 600 (1.572 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.2394\n",
      "INFO:tensorflow:loss = 22.2225, step = 700 (1.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.2188\n",
      "INFO:tensorflow:loss = 20.11082, step = 800 (1.636 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.0382\n",
      "INFO:tensorflow:loss = 13.12772, step = 900 (1.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.235\n",
      "INFO:tensorflow:loss = 17.40796, step = 1000 (1.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.4888\n",
      "INFO:tensorflow:loss = 12.736837, step = 1100 (1.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.3768\n",
      "INFO:tensorflow:loss = 27.182951, step = 1200 (1.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.8514\n",
      "INFO:tensorflow:loss = 6.5443053, step = 1300 (1.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.2161\n",
      "INFO:tensorflow:loss = 17.281004, step = 1400 (1.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.9084\n",
      "INFO:tensorflow:loss = 18.73842, step = 1500 (1.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.4974\n",
      "INFO:tensorflow:loss = 13.060883, step = 1600 (1.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.6718\n",
      "INFO:tensorflow:loss = 6.353765, step = 1700 (1.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.1527\n",
      "INFO:tensorflow:loss = 9.815592, step = 1800 (1.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.3711\n",
      "INFO:tensorflow:loss = 5.413851, step = 1900 (1.584 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.853\n",
      "INFO:tensorflow:loss = 11.270531, step = 2000 (1.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.9967\n",
      "INFO:tensorflow:loss = 11.576701, step = 2100 (1.583 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.2683\n",
      "INFO:tensorflow:loss = 7.8116903, step = 2200 (1.557 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.4745\n",
      "INFO:tensorflow:loss = 7.160853, step = 2300 (1.657 sec)\n",
      "INFO:tensorflow:global_step/sec: 58.3196\n",
      "INFO:tensorflow:loss = 3.753869, step = 2400 (1.716 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.6077\n",
      "INFO:tensorflow:loss = 10.613658, step = 2500 (1.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.9939\n",
      "INFO:tensorflow:loss = 3.2483404, step = 2600 (1.616 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.9005\n",
      "INFO:tensorflow:loss = 4.0708113, step = 2700 (1.617 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.8494\n",
      "INFO:tensorflow:loss = 12.161056, step = 2800 (1.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.8768\n",
      "INFO:tensorflow:loss = 10.159848, step = 2900 (1.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.9829\n",
      "INFO:tensorflow:loss = 9.339679, step = 3000 (1.611 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.1418\n",
      "INFO:tensorflow:loss = 4.216847, step = 3100 (1.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.4033\n",
      "INFO:tensorflow:loss = 20.895077, step = 3200 (1.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.2602\n",
      "INFO:tensorflow:loss = 6.0690207, step = 3300 (1.634 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.9366\n",
      "INFO:tensorflow:loss = 19.52416, step = 3400 (1.666 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.9781\n",
      "INFO:tensorflow:loss = 6.5373745, step = 3500 (1.616 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.6221\n",
      "INFO:tensorflow:loss = 9.517408, step = 3600 (1.573 sec)\n",
      "INFO:tensorflow:global_step/sec: 57.6782\n",
      "INFO:tensorflow:loss = 10.53611, step = 3700 (1.733 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.9533\n",
      "INFO:tensorflow:loss = 2.6684525, step = 3800 (1.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.6478\n",
      "INFO:tensorflow:loss = 10.523245, step = 3900 (1.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.8023\n",
      "INFO:tensorflow:loss = 12.470558, step = 4000 (1.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.2489\n",
      "INFO:tensorflow:loss = 8.268527, step = 4100 (1.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.0416\n",
      "INFO:tensorflow:loss = 3.0939398, step = 4200 (1.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.7672\n",
      "INFO:tensorflow:loss = 5.0522327, step = 4300 (1.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.8932\n",
      "INFO:tensorflow:loss = 4.6698365, step = 4400 (1.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.8432\n",
      "INFO:tensorflow:loss = 6.333212, step = 4500 (1.588 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.3085\n",
      "INFO:tensorflow:loss = 3.4778426, step = 4600 (1.607 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.9242\n",
      "INFO:tensorflow:loss = 4.860959, step = 4700 (1.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.6986\n",
      "INFO:tensorflow:loss = 8.353778, step = 4800 (1.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.0365\n",
      "INFO:tensorflow:loss = 15.188741, step = 4900 (1.612 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.1986\n",
      "INFO:tensorflow:loss = 14.615198, step = 5000 (1.583 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.7054\n",
      "INFO:tensorflow:loss = 2.4595134, step = 5100 (1.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.1911\n",
      "INFO:tensorflow:loss = 10.919692, step = 5200 (1.531 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.2283\n",
      "INFO:tensorflow:loss = 4.628795, step = 5300 (1.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.9486\n",
      "INFO:tensorflow:loss = 5.898924, step = 5400 (1.426 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.772\n",
      "INFO:tensorflow:loss = 5.9011693, step = 5500 (1.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.3278\n",
      "INFO:tensorflow:loss = 3.4940038, step = 5600 (1.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.1827\n",
      "INFO:tensorflow:loss = 2.9328566, step = 5700 (1.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.9778\n",
      "INFO:tensorflow:loss = 6.360457, step = 5800 (1.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.5866\n",
      "INFO:tensorflow:loss = 5.4141397, step = 5900 (1.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.6681\n",
      "INFO:tensorflow:loss = 2.2026558, step = 6000 (1.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.1348\n",
      "INFO:tensorflow:loss = 3.352736, step = 6100 (1.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.9391\n",
      "INFO:tensorflow:loss = 3.5466528, step = 6200 (1.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.5732\n",
      "INFO:tensorflow:loss = 4.8296905, step = 6300 (1.621 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.32\n",
      "INFO:tensorflow:loss = 5.177744, step = 6400 (1.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.6128\n",
      "INFO:tensorflow:loss = 11.817714, step = 6500 (1.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.9441\n",
      "INFO:tensorflow:loss = 2.6722155, step = 6600 (1.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.2995\n",
      "INFO:tensorflow:loss = 8.780106, step = 6700 (1.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.4096\n",
      "INFO:tensorflow:loss = 3.7466643, step = 6800 (1.580 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.9409\n",
      "INFO:tensorflow:loss = 3.3229425, step = 6900 (1.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.8476\n",
      "INFO:tensorflow:loss = 7.5840826, step = 7000 (1.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.4266\n",
      "INFO:tensorflow:loss = 3.0558922, step = 7100 (1.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.8046\n",
      "INFO:tensorflow:loss = 3.0663433, step = 7200 (1.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.2923\n",
      "INFO:tensorflow:loss = 0.75059855, step = 7300 (1.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.0919\n",
      "INFO:tensorflow:loss = 5.558611, step = 7400 (1.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.6868\n",
      "INFO:tensorflow:loss = 8.387833, step = 7500 (1.596 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.5935\n",
      "INFO:tensorflow:loss = 12.133748, step = 7600 (1.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.0523\n",
      "INFO:tensorflow:loss = 0.9847347, step = 7700 (1.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 57.8736\n",
      "INFO:tensorflow:loss = 6.4812646, step = 7800 (1.728 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.8785\n",
      "INFO:tensorflow:loss = 2.4294598, step = 7900 (1.646 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.3775\n",
      "INFO:tensorflow:loss = 0.9636093, step = 8000 (1.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.9718\n",
      "INFO:tensorflow:loss = 7.471739, step = 8100 (1.568 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.1039\n",
      "INFO:tensorflow:loss = 13.272029, step = 8200 (1.580 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.4799\n",
      "INFO:tensorflow:loss = 9.702201, step = 8300 (1.504 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.3446\n",
      "INFO:tensorflow:loss = 2.9021292, step = 8400 (1.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.3442\n",
      "INFO:tensorflow:loss = 5.9141526, step = 8500 (1.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.2793\n",
      "INFO:tensorflow:loss = 0.5505783, step = 8600 (1.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.399\n",
      "INFO:tensorflow:loss = 3.0539992, step = 8700 (1.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.6915\n",
      "INFO:tensorflow:loss = 5.536599, step = 8800 (1.620 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.8114\n",
      "INFO:tensorflow:loss = 2.595124, step = 8900 (1.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.4359\n",
      "INFO:tensorflow:loss = 5.580316, step = 9000 (1.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.2205\n",
      "INFO:tensorflow:loss = 2.0216033, step = 9100 (1.553 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.0256\n",
      "INFO:tensorflow:loss = 3.4742055, step = 9200 (1.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.9455\n",
      "INFO:tensorflow:loss = 3.7125268, step = 9300 (1.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.5119\n",
      "INFO:tensorflow:loss = 1.9000691, step = 9400 (1.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.5855\n",
      "INFO:tensorflow:loss = 4.927365, step = 9500 (1.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.803\n",
      "INFO:tensorflow:loss = 3.391676, step = 9600 (1.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.8385\n",
      "INFO:tensorflow:loss = 6.581366, step = 9700 (1.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.4615\n",
      "INFO:tensorflow:loss = 1.3212067, step = 9800 (1.509 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.7389\n",
      "INFO:tensorflow:loss = 5.222097, step = 9900 (1.570 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.656\n",
      "INFO:tensorflow:loss = 8.312326, step = 10000 (1.623 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.3623\n",
      "INFO:tensorflow:loss = 4.815559, step = 10100 (1.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.3016\n",
      "INFO:tensorflow:loss = 4.9207993, step = 10200 (1.443 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.8587\n",
      "INFO:tensorflow:loss = 4.1456985, step = 10300 (1.646 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.6407\n",
      "INFO:tensorflow:loss = 0.5997162, step = 10400 (1.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.7986\n",
      "INFO:tensorflow:loss = 2.8612244, step = 10500 (1.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.8617\n",
      "INFO:tensorflow:loss = 2.6432571, step = 10600 (1.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.506\n",
      "INFO:tensorflow:loss = 9.637709, step = 10700 (1.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.1472\n",
      "INFO:tensorflow:loss = 0.8004737, step = 10800 (1.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.6732\n",
      "INFO:tensorflow:loss = 1.8003932, step = 10900 (1.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.0293\n",
      "INFO:tensorflow:loss = 15.275931, step = 11000 (1.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.5772\n",
      "INFO:tensorflow:loss = 3.0434988, step = 11100 (1.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.0915\n",
      "INFO:tensorflow:loss = 2.2736042, step = 11200 (1.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.3364\n",
      "INFO:tensorflow:loss = 2.5096543, step = 11300 (1.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.23\n",
      "INFO:tensorflow:loss = 3.2037513, step = 11400 (1.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.6415\n",
      "INFO:tensorflow:loss = 0.89438313, step = 11500 (1.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.4826\n",
      "INFO:tensorflow:loss = 3.6028173, step = 11600 (1.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.7696\n",
      "INFO:tensorflow:loss = 3.8366003, step = 11700 (1.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.5619\n",
      "INFO:tensorflow:loss = 2.409309, step = 11800 (1.649 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.6634\n",
      "INFO:tensorflow:loss = 2.299124, step = 11900 (1.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.966\n",
      "INFO:tensorflow:loss = 3.8487628, step = 12000 (1.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.4602\n",
      "INFO:tensorflow:loss = 0.8890373, step = 12100 (1.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.8298\n",
      "INFO:tensorflow:loss = 5.774823, step = 12200 (1.645 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.7684\n",
      "INFO:tensorflow:loss = 0.99029845, step = 12300 (1.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.4476\n",
      "INFO:tensorflow:loss = 12.4673605, step = 12400 (1.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.9541\n",
      "INFO:tensorflow:loss = 5.3935094, step = 12500 (1.670 sec)\n",
      "INFO:tensorflow:global_step/sec: 57.3795\n",
      "INFO:tensorflow:loss = 2.442435, step = 12600 (1.742 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.2398\n",
      "INFO:tensorflow:loss = 2.5731776, step = 12700 (1.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.6191\n",
      "INFO:tensorflow:loss = 1.0474232, step = 12800 (1.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 57.9211\n",
      "INFO:tensorflow:loss = 3.20477, step = 12900 (1.732 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.7379\n",
      "INFO:tensorflow:loss = 4.7918053, step = 13000 (1.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.8931\n",
      "INFO:tensorflow:loss = 10.773488, step = 13100 (1.562 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.9868\n",
      "INFO:tensorflow:loss = 3.5378761, step = 13200 (1.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.4533\n",
      "INFO:tensorflow:loss = 5.578799, step = 13300 (1.501 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.3914\n",
      "INFO:tensorflow:loss = 4.9183307, step = 13400 (1.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.4661\n",
      "INFO:tensorflow:loss = 4.244455, step = 13500 (1.627 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.7422\n",
      "INFO:tensorflow:loss = 2.7834928, step = 13600 (1.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.8659\n",
      "INFO:tensorflow:loss = 6.367489, step = 13700 (1.565 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.216\n",
      "INFO:tensorflow:loss = 3.7311885, step = 13800 (1.558 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.539\n",
      "INFO:tensorflow:loss = 3.7149596, step = 13900 (1.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.6087\n",
      "INFO:tensorflow:loss = 1.1633126, step = 14000 (1.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.5642\n",
      "INFO:tensorflow:loss = 2.208962, step = 14100 (1.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.5239\n",
      "INFO:tensorflow:loss = 5.4077806, step = 14200 (1.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.1118\n",
      "INFO:tensorflow:loss = 7.823989, step = 14300 (1.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.7616\n",
      "INFO:tensorflow:loss = 1.7678924, step = 14400 (1.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.0538\n",
      "INFO:tensorflow:loss = 1.1833795, step = 14500 (1.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.0972\n",
      "INFO:tensorflow:loss = 2.40641, step = 14600 (1.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.678\n",
      "INFO:tensorflow:loss = 2.0496764, step = 14700 (1.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.6092\n",
      "INFO:tensorflow:loss = 3.4919062, step = 14800 (1.599 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.3509\n",
      "INFO:tensorflow:loss = 0.99685156, step = 14900 (1.552 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 15000 into /tmp/tmp3isqf8hw/bow_embeddings/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.0413806.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-10-19-16:31:54\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp3isqf8hw/bow_embeddings/model.ckpt-15000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-10-19-16:31:57\n",
      "INFO:tensorflow:Saving dict for global step 15000: accuracy = 0.96485096, accuracy_baseline = 0.66350985, auc = 0.9901126, auc_precision_recall = 0.99503237, average_loss = 0.11875925, global_step = 15000, label/mean = 0.66350985, loss = 11.825565, precision = 0.976296, prediction/mean = 0.6563753, recall = 0.97059107\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 15000: /tmp/tmp3isqf8hw/bow_embeddings/model.ckpt-15000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp3isqf8hw/bow_embeddings/model.ckpt-15000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.DNNClassifier(\n",
    "    hidden_units=[100],\n",
    "    feature_columns=[word_embedding_column], \n",
    "    model_dir=os.path.join(model_dir, 'bow_embeddings'))\n",
    "\n",
    "train_and_evaluate(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8qQep5feWei",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Podemos usar TensorBoard para visualizar los vectores que aprendió el modelos proyectados a $\\mathbb{R}^3$ usando *t-SNE*. El siguiente código genera un archivo con los labels que podemos cargar para visualizar que vector corresponde a cada palabra.\n",
    "\n",
    "![Embedding image](https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/embeddings.gif?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "IM1Nizn0fUmF",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#@markdown Descargar archivo de labels para los vectores\n",
    "from google.colab import files\n",
    "with open(os.path.join(model_dir, 'metadata.tsv'), 'w', encoding=\"utf-8\") as f:\n",
    "    for i in range(index_offset + 1):\n",
    "        f.write('<_{}_>\\n'.format(i))\n",
    "    for index in range(1, vocab_size - index_offset):\n",
    "        f.write(word_inverted_index[index] + '\\n')\n",
    "files.download(os.path.join(model_dir, 'metadata.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fRxL7UxsfhXO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convoluciones\n",
    "\n",
    "Las convoluciones apuntan a salirnos del modelo de bolsa de palabras para poder procesar ventanas de texto dentro de un documento.\n",
    "\n",
    "![cnn](https://cdn-images-1.medium.com/max/1000/1*TsW55MIvzHwb-GRA21Q7Zw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EUBBaR-Gg6Vq",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_cnn_logits(features, training, params):\n",
    "  \n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, embedding_size,\n",
    "        trainable=params['embedding_trainable'],\n",
    "        initializer=params['embedding_initializer'])\n",
    "  \n",
    "    dropout_emb = tf.layers.dropout(inputs=input_layer, \n",
    "                                    rate=0.2, \n",
    "                                    training=training)\n",
    "\n",
    "    conv = tf.layers.conv1d(\n",
    "        inputs=dropout_emb,\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    pool = tf.reduce_max(input_tensor=conv, axis=1) # Global Max Pooling\n",
    "    \n",
    "    hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu)\n",
    "    \n",
    "    dropout_hidden = tf.layers.dropout(inputs=hidden, \n",
    "                                       rate=0.2, \n",
    "                                       training=training)\n",
    "    \n",
    "    return tf.layers.dense(inputs=dropout_hidden, units=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvcEbNgFisdX",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](https://raw.githubusercontent.com/eisenjulian/nlp_estimator_tutorial/master/mermaid/cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvcEbNgFisdX",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Un estimador se define a partir de una función `model_fn(features, labels, mode, params)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oa87AvyYkKBa",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "head = tf.contrib.estimator.binary_classification_head()\n",
    "\n",
    "def cnn_model_fn(features, labels, mode, params):    \n",
    "    training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    \n",
    "    logits = get_cnn_logits(features, training, params)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    def _train_op_fn(loss):\n",
    "        return optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "\n",
    "    return head.create_estimator_spec(\n",
    "        features=features,\n",
    "        labels=tf.reshape(labels, [-1, 1]) if labels is not None else None,\n",
    "        mode=mode,\n",
    "        logits=logits, \n",
    "        train_op_fn=_train_op_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Px5hERDqj_Zi",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'embedding_initializer': tf.random_uniform_initializer(-1.0, 1.0),\n",
    "    'embedding_trainable': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2288
    },
    "colab_type": "code",
    "id": "eq67F73TNrNw",
    "outputId": "ccca545e-de73-46e1-f5de-d932833e90fc",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp43x26c8n/cnn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7febcc9d4588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp43x26c8n/cnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.680052, step = 0\n",
      "INFO:tensorflow:global_step/sec: 70.0503\n",
      "INFO:tensorflow:loss = 0.47510922, step = 100 (1.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.956\n",
      "INFO:tensorflow:loss = 0.27631557, step = 200 (0.992 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.494\n",
      "INFO:tensorflow:loss = 0.2799267, step = 300 (0.986 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.6904\n",
      "INFO:tensorflow:loss = 0.17059124, step = 400 (1.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.784\n",
      "INFO:tensorflow:loss = 0.22372505, step = 500 (0.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.977\n",
      "INFO:tensorflow:loss = 0.21556525, step = 600 (0.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.659\n",
      "INFO:tensorflow:loss = 0.14175622, step = 700 (0.971 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.495\n",
      "INFO:tensorflow:loss = 0.2012616, step = 800 (1.061 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.662\n",
      "INFO:tensorflow:loss = 0.14043884, step = 900 (0.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.941\n",
      "INFO:tensorflow:loss = 0.12712735, step = 1000 (0.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.629\n",
      "INFO:tensorflow:loss = 0.06170453, step = 1100 (0.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.5408\n",
      "INFO:tensorflow:loss = 0.08751446, step = 1200 (1.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.213\n",
      "INFO:tensorflow:loss = 0.09954479, step = 1300 (0.981 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.658\n",
      "INFO:tensorflow:loss = 0.13391668, step = 1400 (0.995 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.9899\n",
      "INFO:tensorflow:loss = 0.06784996, step = 1500 (1.052 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.937\n",
      "INFO:tensorflow:loss = 0.035811577, step = 1600 (0.969 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.723\n",
      "INFO:tensorflow:loss = 0.052976076, step = 1700 (0.976 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.702\n",
      "INFO:tensorflow:loss = 0.07059609, step = 1800 (0.993 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.6641\n",
      "INFO:tensorflow:loss = 0.07970143, step = 1900 (1.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.713\n",
      "INFO:tensorflow:loss = 0.081439406, step = 2000 (0.985 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.839\n",
      "INFO:tensorflow:loss = 0.20994079, step = 2100 (0.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.07\n",
      "INFO:tensorflow:loss = 0.123395205, step = 2200 (0.987 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.3438\n",
      "INFO:tensorflow:loss = 0.11780539, step = 2300 (1.048 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.757\n",
      "INFO:tensorflow:loss = 0.034418624, step = 2400 (0.976 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.182\n",
      "INFO:tensorflow:loss = 0.06929264, step = 2500 (0.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.4638\n",
      "INFO:tensorflow:loss = 0.06800615, step = 2600 (1.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.676\n",
      "INFO:tensorflow:loss = 0.10102424, step = 2700 (0.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.255\n",
      "INFO:tensorflow:loss = 0.013503251, step = 2800 (0.980 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.893\n",
      "INFO:tensorflow:loss = 0.054088622, step = 2900 (0.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.6083\n",
      "INFO:tensorflow:loss = 0.07858383, step = 3000 (1.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.28\n",
      "INFO:tensorflow:loss = 0.028483577, step = 3100 (0.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.79\n",
      "INFO:tensorflow:loss = 0.1469439, step = 3200 (0.989 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.639\n",
      "INFO:tensorflow:loss = 0.026200198, step = 3300 (0.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.7867\n",
      "INFO:tensorflow:loss = 0.091127984, step = 3400 (1.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.947\n",
      "INFO:tensorflow:loss = 0.04215462, step = 3500 (0.980 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.69\n",
      "INFO:tensorflow:loss = 0.068557635, step = 3600 (0.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.5868\n",
      "INFO:tensorflow:loss = 0.03882247, step = 3700 (1.049 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.809\n",
      "INFO:tensorflow:loss = 0.054420844, step = 3800 (0.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.55\n",
      "INFO:tensorflow:loss = 0.02942058, step = 3900 (0.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.657\n",
      "INFO:tensorflow:loss = 0.041881777, step = 4000 (0.971 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.344\n",
      "INFO:tensorflow:loss = 0.040109724, step = 4100 (1.064 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.964\n",
      "INFO:tensorflow:loss = 0.10299821, step = 4200 (0.978 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.427\n",
      "INFO:tensorflow:loss = 0.10268093, step = 4300 (0.968 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.771\n",
      "INFO:tensorflow:loss = 0.033030998, step = 4400 (0.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.8262\n",
      "INFO:tensorflow:loss = 0.0207128, step = 4500 (1.046 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.395\n",
      "INFO:tensorflow:loss = 0.036400355, step = 4600 (0.987 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.814\n",
      "INFO:tensorflow:loss = 0.049988117, step = 4700 (0.979 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.2373\n",
      "INFO:tensorflow:loss = 0.023613958, step = 4800 (1.053 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.297\n",
      "INFO:tensorflow:loss = 0.023228742, step = 4900 (0.964 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/tmp43x26c8n/cnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.116264455.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-10-20-03:43:40\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp43x26c8n/cnn/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-10-20-03:43:41\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.9676476, accuracy_baseline = 0.66350985, auc = 0.99152285, auc_precision_recall = 0.9956374, average_loss = 0.103014424, global_step = 5000, label/mean = 0.66350985, loss = 0.102679975, precision = 0.96875, prediction/mean = 0.6722226, recall = 0.98294854\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /tmp/tmp43x26c8n/cnn/model.ckpt-5000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp43x26c8n/cnn/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "cnn_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                                        model_dir=os.path.join(model_dir, 'cnn'),\n",
    "                                        params=params)\n",
    "train_and_evaluate(cnn_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Fk4PcH2hUsl",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transferencia de aprendizaje\n",
    "\n",
    "Inicializamos el modelos con vectores random, por lo que debemos aprender a *hablar* de cero cada vez. Si usamos vectores pre-entranados podemos:\n",
    "\n",
    " * Aprender de datos sin etiquetar\n",
    " * Aprovechar mejor la poca data etiquetada que tenemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "puFrK72lhf8y",
    "outputId": "1963f173-ff07-4920-d545-3950f825ccb6",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-10-20 03:54:13--  https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.es.vec\n",
      "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 54.231.237.37\n",
      "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|54.231.237.37|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2594302560 (2.4G) [binary/octet-stream]\n",
      "Saving to: ‘wiki.es.vec’\n",
      "\n",
      "wiki.es.vec         100%[===================>]   2.42G  62.7MB/s    in 41s     \n",
      "\n",
      "2018-10-20 03:54:54 (60.0 MB/s) - ‘wiki.es.vec’ saved [2594302560/2594302560]\n",
      "\n",
      "Successfully loaded pretrained embeddings for 4809/5000 words.\n"
     ]
    }
   ],
   "source": [
    "#@title Cargar vectores pre-entrenados\n",
    "#@markdown `embedding_matrix = load_fasttext_embeddings('wiki.es.vec')`\n",
    "\n",
    "if not os.path.exists('wiki.es.vec'):\n",
    "    ! wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.es.vec\n",
    "      \n",
    "def load_fasttext_embeddings(path):\n",
    "    embeddings = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            if not values[1][0] in '0123456789-':\n",
    "                continue\n",
    "            w = values[0]\n",
    "            if w not in word_index or word_index[w] >= vocab_size:\n",
    "                continue\n",
    "            vectors = np.asarray(values[1:51], dtype='float32')\n",
    "            embeddings[w] = vectors\n",
    "\n",
    "    embedding_matrix = np.random.uniform(-1, 1, size=(vocab_size, embedding_size))\n",
    "    num_loaded = 0\n",
    "    for w, i in word_index.items():\n",
    "        v = embeddings.get(w)\n",
    "        if v is not None and i + index_offset < vocab_size:\n",
    "            embedding_matrix[i + index_offset] = v\n",
    "            num_loaded += 1\n",
    "    print('Successfully loaded pretrained embeddings for '\n",
    "          f'{num_loaded}/{vocab_size} words.')\n",
    "    embedding_matrix = embedding_matrix.astype(np.float32)\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = load_fasttext_embeddings('wiki.es.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaTrA0V9iLdF",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ahora podemos cargar los vectores en nuestro modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2288
    },
    "colab_type": "code",
    "id": "I6x6Lyu-iL3b",
    "outputId": "cba074ae-2727-44af-d12e-39764915cdfc",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp43x26c8n/cnn_pretrained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7febc86692b0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp43x26c8n/cnn_pretrained/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.84539866, step = 0\n",
      "INFO:tensorflow:global_step/sec: 93.0393\n",
      "INFO:tensorflow:loss = 0.5461022, step = 100 (1.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.133\n",
      "INFO:tensorflow:loss = 0.51779944, step = 200 (0.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.757\n",
      "INFO:tensorflow:loss = 0.46507388, step = 300 (0.785 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.249\n",
      "INFO:tensorflow:loss = 0.35241744, step = 400 (0.875 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.641\n",
      "INFO:tensorflow:loss = 0.49458024, step = 500 (0.786 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.335\n",
      "INFO:tensorflow:loss = 0.38376114, step = 600 (0.795 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.024\n",
      "INFO:tensorflow:loss = 0.38960597, step = 700 (0.790 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.774\n",
      "INFO:tensorflow:loss = 0.36956573, step = 800 (0.866 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.831\n",
      "INFO:tensorflow:loss = 0.34900916, step = 900 (0.786 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.157\n",
      "INFO:tensorflow:loss = 0.45650786, step = 1000 (0.793 sec)\n",
      "INFO:tensorflow:global_step/sec: 122.692\n",
      "INFO:tensorflow:loss = 0.33210593, step = 1100 (0.817 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.643\n",
      "INFO:tensorflow:loss = 0.3216016, step = 1200 (0.881 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.246\n",
      "INFO:tensorflow:loss = 0.32885924, step = 1300 (0.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.643\n",
      "INFO:tensorflow:loss = 0.29388353, step = 1400 (0.789 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.324\n",
      "INFO:tensorflow:loss = 0.33920658, step = 1500 (0.881 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.255\n",
      "INFO:tensorflow:loss = 0.29709136, step = 1600 (0.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.81\n",
      "INFO:tensorflow:loss = 0.25715777, step = 1700 (0.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.806\n",
      "INFO:tensorflow:loss = 0.24653597, step = 1800 (0.790 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.138\n",
      "INFO:tensorflow:loss = 0.25138462, step = 1900 (0.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.099\n",
      "INFO:tensorflow:loss = 0.2623765, step = 2000 (0.786 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.815\n",
      "INFO:tensorflow:loss = 0.20875736, step = 2100 (0.786 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.563\n",
      "INFO:tensorflow:loss = 0.25374708, step = 2200 (0.786 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.792\n",
      "INFO:tensorflow:loss = 0.31222907, step = 2300 (0.865 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.775\n",
      "INFO:tensorflow:loss = 0.22037698, step = 2400 (0.787 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.298\n",
      "INFO:tensorflow:loss = 0.28768107, step = 2500 (0.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.666\n",
      "INFO:tensorflow:loss = 0.1416176, step = 2600 (0.861 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.038\n",
      "INFO:tensorflow:loss = 0.19999933, step = 2700 (0.777 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.818\n",
      "INFO:tensorflow:loss = 0.23147206, step = 2800 (0.784 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.974\n",
      "INFO:tensorflow:loss = 0.30802009, step = 2900 (0.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.433\n",
      "INFO:tensorflow:loss = 0.237066, step = 3000 (0.856 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.064\n",
      "INFO:tensorflow:loss = 0.16562876, step = 3100 (0.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.93\n",
      "INFO:tensorflow:loss = 0.22199558, step = 3200 (0.787 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.604\n",
      "INFO:tensorflow:loss = 0.2401706, step = 3300 (0.789 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.702\n",
      "INFO:tensorflow:loss = 0.18543217, step = 3400 (0.861 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.346\n",
      "INFO:tensorflow:loss = 0.15679185, step = 3500 (0.786 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.506\n",
      "INFO:tensorflow:loss = 0.26153693, step = 3600 (0.774 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.295\n",
      "INFO:tensorflow:loss = 0.26375306, step = 3700 (0.872 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.971\n",
      "INFO:tensorflow:loss = 0.2613709, step = 3800 (0.785 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.971\n",
      "INFO:tensorflow:loss = 0.20221515, step = 3900 (0.787 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.093\n",
      "INFO:tensorflow:loss = 0.3793162, step = 4000 (0.792 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.638\n",
      "INFO:tensorflow:loss = 0.18671066, step = 4100 (0.868 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.313\n",
      "INFO:tensorflow:loss = 0.1662364, step = 4200 (0.782 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.564\n",
      "INFO:tensorflow:loss = 0.15407836, step = 4300 (0.788 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.87\n",
      "INFO:tensorflow:loss = 0.17024824, step = 4400 (0.797 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.265\n",
      "INFO:tensorflow:loss = 0.19531636, step = 4500 (0.856 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.309\n",
      "INFO:tensorflow:loss = 0.32938617, step = 4600 (0.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 126.536\n",
      "INFO:tensorflow:loss = 0.22784133, step = 4700 (0.791 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.778\n",
      "INFO:tensorflow:loss = 0.11575761, step = 4800 (0.867 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.238\n",
      "INFO:tensorflow:loss = 0.28022024, step = 4900 (0.776 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/tmp43x26c8n/cnn_pretrained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.15853792.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-10-20-03:56:29\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp43x26c8n/cnn_pretrained/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-10-20-03:56:31\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.9352952, accuracy_baseline = 0.66350985, auc = 0.98205316, auc_precision_recall = 0.99064255, average_loss = 0.16114591, global_step = 5000, label/mean = 0.66350985, loss = 0.16180901, precision = 0.9521067, prediction/mean = 0.65339386, recall = 0.9502826\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /tmp/tmp43x26c8n/cnn_pretrained/model.ckpt-5000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp43x26c8n/cnn_pretrained/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "def my_initializer(shape=None, dtype=tf.float32, partition_info=None):\n",
    "    return embedding_matrix\n",
    "\n",
    "params = {'embedding_initializer': my_initializer, 'embedding_trainable': False}\n",
    "\n",
    "cnn_pretrained_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "    model_dir=os.path.join(model_dir, 'cnn_pretrained'),\n",
    "    params=params)\n",
    "train_and_evaluate(cnn_pretrained_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AWZ833DXlhPc",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Podemos correr este modelo dejando re-entrenar los pesos o no. También hay papers que muestran que lo mejor es regalurazar hacie esos valores en lugar de solo usarlos como inicialización. En la práctica salvo excepciones dejarlos fijos es lo mejor para evitar overfitting, las mejoras que se obtienen son marginales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2288
    },
    "colab_type": "code",
    "id": "hX8S0ZahmqHv",
    "outputId": "77408520-8f25-4946-8381-334d7c48bf71",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp43x26c8n/cnn_pretrained_retrain', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7febc8669588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp43x26c8n/cnn_pretrained_retrain/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.6951174, step = 0\n",
      "INFO:tensorflow:global_step/sec: 72.5242\n",
      "INFO:tensorflow:loss = 0.5510155, step = 100 (1.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.329\n",
      "INFO:tensorflow:loss = 0.2883177, step = 200 (0.965 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.762\n",
      "INFO:tensorflow:loss = 0.29106748, step = 300 (0.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.7812\n",
      "INFO:tensorflow:loss = 0.08424552, step = 400 (1.042 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.271\n",
      "INFO:tensorflow:loss = 0.12632294, step = 500 (0.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.03\n",
      "INFO:tensorflow:loss = 0.13466452, step = 600 (0.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.167\n",
      "INFO:tensorflow:loss = 0.09175409, step = 700 (0.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.7168\n",
      "INFO:tensorflow:loss = 0.08976228, step = 800 (1.051 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.605\n",
      "INFO:tensorflow:loss = 0.15756254, step = 900 (0.975 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.812\n",
      "INFO:tensorflow:loss = 0.05402628, step = 1000 (0.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.441\n",
      "INFO:tensorflow:loss = 0.17868428, step = 1100 (0.976 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.6012\n",
      "INFO:tensorflow:loss = 0.06232211, step = 1200 (1.060 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.071\n",
      "INFO:tensorflow:loss = 0.10755536, step = 1300 (0.968 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.918\n",
      "INFO:tensorflow:loss = 0.075015604, step = 1400 (0.965 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.9879\n",
      "INFO:tensorflow:loss = 0.06914012, step = 1500 (1.053 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.906\n",
      "INFO:tensorflow:loss = 0.09351297, step = 1600 (0.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.583\n",
      "INFO:tensorflow:loss = 0.08560257, step = 1700 (0.990 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.491\n",
      "INFO:tensorflow:loss = 0.031517867, step = 1800 (0.979 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.9492\n",
      "INFO:tensorflow:loss = 0.025956629, step = 1900 (1.054 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.094\n",
      "INFO:tensorflow:loss = 0.061348774, step = 2000 (0.959 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.996\n",
      "INFO:tensorflow:loss = 0.10493286, step = 2100 (0.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.579\n",
      "INFO:tensorflow:loss = 0.026749136, step = 2200 (0.975 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.6816\n",
      "INFO:tensorflow:loss = 0.10399229, step = 2300 (1.053 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.494\n",
      "INFO:tensorflow:loss = 0.081840456, step = 2400 (0.966 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.067\n",
      "INFO:tensorflow:loss = 0.019798515, step = 2500 (0.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.6092\n",
      "INFO:tensorflow:loss = 0.07128025, step = 2600 (1.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.573\n",
      "INFO:tensorflow:loss = 0.051482927, step = 2700 (0.978 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.001\n",
      "INFO:tensorflow:loss = 0.015841817, step = 2800 (0.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.822\n",
      "INFO:tensorflow:loss = 0.060753316, step = 2900 (0.969 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.4152\n",
      "INFO:tensorflow:loss = 0.011977395, step = 3000 (1.048 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.431\n",
      "INFO:tensorflow:loss = 0.01763977, step = 3100 (0.980 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.559\n",
      "INFO:tensorflow:loss = 0.0558972, step = 3200 (0.964 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.119\n",
      "INFO:tensorflow:loss = 0.030268917, step = 3300 (0.968 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.7005\n",
      "INFO:tensorflow:loss = 0.05114956, step = 3400 (1.058 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.262\n",
      "INFO:tensorflow:loss = 0.07019523, step = 3500 (0.951 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.113\n",
      "INFO:tensorflow:loss = 0.02052544, step = 3600 (0.951 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.0922\n",
      "INFO:tensorflow:loss = 0.0031379228, step = 3700 (1.053 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.803\n",
      "INFO:tensorflow:loss = 0.005941484, step = 3800 (0.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.399\n",
      "INFO:tensorflow:loss = 0.011415375, step = 3900 (0.964 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.597\n",
      "INFO:tensorflow:loss = 0.11735851, step = 4000 (0.976 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.044\n",
      "INFO:tensorflow:loss = 0.004495168, step = 4100 (1.049 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.725\n",
      "INFO:tensorflow:loss = 0.0109657515, step = 4200 (0.986 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.836\n",
      "INFO:tensorflow:loss = 0.00474798, step = 4300 (0.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.058\n",
      "INFO:tensorflow:loss = 0.07937584, step = 4400 (0.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.415\n",
      "INFO:tensorflow:loss = 0.0065729683, step = 4500 (1.050 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.59\n",
      "INFO:tensorflow:loss = 0.007971721, step = 4600 (0.971 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.689\n",
      "INFO:tensorflow:loss = 0.0096620545, step = 4700 (0.976 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.374\n",
      "INFO:tensorflow:loss = 0.0070715807, step = 4800 (1.046 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.575\n",
      "INFO:tensorflow:loss = 0.068367176, step = 4900 (0.966 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/tmp43x26c8n/cnn_pretrained_retrain/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.054352336.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-10-20-03:58:32\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp43x26c8n/cnn_pretrained_retrain/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-10-20-03:58:34\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.96561366, accuracy_baseline = 0.66350985, auc = 0.9897962, auc_precision_recall = 0.9950244, average_loss = 0.124364, global_step = 5000, label/mean = 0.66350985, loss = 0.12459737, precision = 0.98029894, prediction/mean = 0.6531759, recall = 0.96762145\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /tmp/tmp43x26c8n/cnn_pretrained_retrain/model.ckpt-5000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp43x26c8n/cnn_pretrained_retrain/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "params = {'embedding_initializer': my_initializer, 'embedding_trainable': True}\n",
    "cnn_pretrained_classifier_reatrain = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "    model_dir=os.path.join(model_dir, 'cnn_pretrained_retrain'),\n",
    "    params=params)\n",
    "train_and_evaluate(cnn_pretrained_classifier_reatrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IEFLHohZlvcF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTMs\n",
    "\n",
    "LSTMs son el más popular algoritmo de red neuronal recursiva para procesar secuencias. Una de las desventajas al comparar con CNNs es que, por la naturaleza de la recursión los modelos son mas profundos y complejos, que resulta en tiempos mas lentos de entrenamiento y peor convergencia.\n",
    "\n",
    "![LSTM Architecture](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qV2nwG_Wo5_d",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_lstm_logits(features):\n",
    "    # [batch_size x sentence_size x embedding_size]\n",
    "    inputs = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, embedding_size,\n",
    "        initializer=tf.random_uniform_initializer(-1.0, 1.0))\n",
    "\n",
    "    # create an LSTM cell of size 100\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(100)\n",
    "    \n",
    "    # create the complete LSTM\n",
    "    _, final_states = tf.nn.dynamic_rnn(\n",
    "        lstm_cell, inputs, sequence_length=features['len'], dtype=tf.float32)\n",
    "\n",
    "    # get the final hidden states of dimensionality [batch_size x sentence_size]\n",
    "    outputs = final_states.h\n",
    "\n",
    "    return tf.layers.dense(inputs=outputs, units=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "cT8puam9oPHu",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#@markdown Definir `lstm_model_fn`\n",
    "\n",
    "head = tf.contrib.estimator.binary_classification_head()\n",
    "\n",
    "def lstm_model_fn(features, labels, mode):    \n",
    "    \n",
    "    logits = get_lstm_logits(features)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "    def _train_op_fn(loss):\n",
    "        return optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "\n",
    "    return head.create_estimator_spec(\n",
    "        features=features,\n",
    "        labels=tf.reshape(labels, [-1, 1]) if labels is not None else None,\n",
    "        mode=mode,\n",
    "        logits=logits,\n",
    "        train_op_fn=_train_op_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "2zII4JkXrgGd",
    "outputId": "c8a96e65-21ad-494f-e244-94a7ac3669be",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp43x26c8n/lstm', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7febd387d630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp43x26c8n/lstm/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/tmp43x26c8n/lstm/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0011096846, step = 5000\n",
      "INFO:tensorflow:global_step/sec: 3.52459\n",
      "INFO:tensorflow:loss = 0.05123483, step = 5100 (28.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.89591\n",
      "INFO:tensorflow:loss = 0.003624744, step = 5200 (25.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.84121\n",
      "INFO:tensorflow:loss = 0.008583602, step = 5300 (26.038 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.68614\n",
      "INFO:tensorflow:loss = 0.00045231025, step = 5400 (27.127 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.83717\n",
      "INFO:tensorflow:loss = 0.00070083974, step = 5500 (26.059 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.74824\n",
      "INFO:tensorflow:loss = 0.0052617956, step = 5600 (26.683 sec)\n"
     ]
    }
   ],
   "source": [
    "lstm_classifier = tf.estimator.Estimator(model_fn=lstm_model_fn,\n",
    "                                         model_dir=os.path.join(model_dir, 'lstm'))\n",
    "train_and_evaluate(lstm_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1APgAb5iVkTU",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los **LSTM Networks** se han vuelto el caballito de batalla en muchos problemas de NLP, usando el output de cada unidad para armar módelos de lenguaje o la clasificar cada token de una oración.\n",
    " \n",
    "Estos mismos módelos se pueden usar después de forma inversa para **generar texto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HP09wahLnkVE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resultados\n",
    "\n",
    "Para ir cerrando, lo primero es como usar esto para sacar predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "KywWbhMqJwX2",
    "outputId": "5d5bdd1e-c29a-42da-e6f4-f7f83ac2fda8",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La comida daba asco, estaba dura la carne, horrible!\n",
      "\t0.000395 bow_sparse\n",
      "\t0.000291 bow_sparse\n",
      "\t0.000000 cnn\n",
      "\t0.073282 cnn_pretrained\n",
      "\t0.000000 cnn_pretrained_retrain\n",
      "Estaba todo riquísimo, vale la pena\n",
      "\t0.946516 bow_sparse\n",
      "\t0.933265 bow_sparse\n",
      "\t0.999999 cnn\n",
      "\t0.972757 cnn_pretrained\n",
      "\t0.999995 cnn_pretrained_retrain\n"
     ]
    }
   ],
   "source": [
    "#@title Making Predictions { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
    "doc1 = \"La comida daba asco, estaba dura la carne, horrible!\" #@param {type:\"string\"}\n",
    "doc2 = \"Estaba todo riquísimo, vale la pena\" #@param {type:\"string\"}\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "def print_predictions(sentences):\n",
    "    indexes = [doc_to_index(sentence) for sentence in sentences]\n",
    "    x = sequence.pad_sequences(indexes, \n",
    "                               maxlen=sentence_size, \n",
    "                               padding='post', \n",
    "                               value=-1)\n",
    "    length = np.array([min(len(x), sentence_size) for x in indexes])\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": x, \"len\": length}, shuffle=False)\n",
    "    predictions = {}\n",
    "    for path, classifier in all_classifiers.items():\n",
    "        predictions[path] = [p['logistic'][0] for p in classifier.predict(input_fn=predict_input_fn)]\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        print(sentence)\n",
    "        for path in all_classifiers:\n",
    "            print(\"\\t{0:.6f} {1}\".format(predictions[path][idx].item(), os.path.basename(path)))\n",
    "            \n",
    "print_predictions([doc1, doc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pfSHGGb_nskl",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Corriendo TensorBoard\n",
    "\n",
    "Ahora podemos correr TensorBoard y ver como los diferentes modelso que entramos se comparan entre sí en terminos de tiempo de entrenamiento y perforamnce.\n",
    "\n",
    "En la terminal\n",
    "\n",
    "> tensorboard --logdir={model_dir}\n",
    "\n",
    "Podemos ver varias métricas durante entrenamiento y prueba, incluída la loss function de cada modelos durante el entenamiento y las PR curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ugkBUHVEkP-h",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Otras herramientas\n",
    "\n",
    "Otra herramienta muy útil es [Spacy](https://spacy.io) a la hora de trabajar sobre texto. Veamos que puede hacer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fc7Q0tVBUzpr"
   },
   "outputs": [],
   "source": [
    "! pip install -U spacy\n",
    "! python -m spacy download es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7uhhuaJ3WaBe"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es')\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "rU-8M2OzXj--"
   },
   "outputs": [],
   "source": [
    "#@title Análisis de dependencias\n",
    "sentence = \"Los propietarios me trataron muy bien\" #@param {type:\"string\"}\n",
    "doc = nlp(sentence)\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "XzXfwgamki1i"
   },
   "outputs": [],
   "source": [
    "#@title Análisis de entidades\n",
    "sentence = \"La comida India era muy rica y estaba comiendo Fidel Castro.\" #@param {type:\"string\"}\n",
    "doc = nlp(sentence)\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OUSRtAxDlMgn"
   },
   "source": [
    "También podemos buscar en un corpus grande frases que matchean ciertos patrones. Muy útil a la hora hacer benchmarks para datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjZM0kQhbtdE"
   },
   "outputs": [],
   "source": [
    "matched_sents = [] # collect data of matched sentences to be visualized\n",
    "matcher = Matcher(nlp.vocab)\n",
    "reviews = '\\n'.join(list(get_docs('*'))[0:1000])\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start : end] # matched span\n",
    "    sent = span.sent # sentence containing matched span\n",
    "    # append mock entity for match in displaCy style to matched_sents\n",
    "    # get the match span by ofsetting the start and end of the span with the\n",
    "    # start and end of the sentence in the doc\n",
    "    match_ents = [{'start': span.start_char - sent.start_char,\n",
    "                   'end': span.end_char - sent.start_char,\n",
    "                   'label': 'MATCH'}]\n",
    "    matched_sents.append({'text': sent.text, 'ents': match_ents })\n",
    "\n",
    "pattern = [{'LOWER': 'comida'}, {'LEMMA': 'ser'}, {'POS': 'ADV', 'OP': '*'},\n",
    "           {'POS': 'ADJ'}]\n",
    "matcher.add('La comida', collect_sents, pattern) # add pattern\n",
    "matches = matcher(nlp(reviews)) # match on your text\n",
    "displacy.render(matched_sents, manual=True, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxLZKRrioNHV",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "* [Natural Language Processing (almost) from Scratch](https://arxiv.org/abs/1103.0398) - *Paper fundacional a la hora de aplicar DNN a problemas de NLP*\n",
    " \n",
    "* [Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks by Severyn et al. 2015](https://www.semanticscholar.org/paper/Learning-to-Rank-Short-Text-Pairs-with-Deep-Neural-Severyn-Moschitti/452f7411af7d471dd3ba84c2b06b2aaffc38cdb9) - *Usa CNNs para generar representaciones de los textos*\n",
    "\n",
    "* [Bag of Tricks for Efficient Text Classification\n",
    "](https://arxiv.org/abs/1607.01759) - *Explica el modelos de fast text: buen transfer learning + ngram features + hierarchical softmak le gana muchos modelos mas complejos*\n",
    "\n",
    "* [FastText pretrained vectors](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md)\n",
    " \n",
    "* [Efficient Estimation of Word Representations in Vector Space\n",
    "](https://arxiv.org/abs/1301.3781) -*Introduce los tests para medir la calidad de pesos a transferir*\n",
    " \n",
    "* [Procesamiento de Lenguaje Natural en\n",
    "Sistemas de Análisis de Sentimientos](http://materias.fi.uba.ar/7500/Dubiau.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-d1ZLisVrKjY",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "  <br><br><br><br>\n",
    "  <h1>¡Muchas gracias!</h1>\n",
    "  <h2>¿Preguntas? ¿CVs?</h2>\n",
    "  <br><br><br><br>\n",
    "</center>\n",
    "<table width=\"100%\"><tbody><tr aligh=\"left\">\n",
    "      <td><img width=\"100\" src=\"https://storage.googleapis.com/m-infra.appspot.com/public/botmaker/bmtop7.png\"></td>\n",
    "      <td align=\"right\">20 de Octube de 2018 - DevFest Buenos Aires</td>\n",
    "</tr></tbody></table>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "DevFest 2018 - Clasificación de Texto en TensorFlow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
